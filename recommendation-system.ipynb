{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5239462,"sourceType":"datasetVersion","datasetId":3020336}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: inspect dataset files\nimport os, glob\nbase = \"/kaggle/input\"\nprint(\"Top-level folders in /kaggle/input:\")\nprint(os.listdir(base))\n\n# try to find the amazon dataset folder\nfor d in os.listdir(base):\n    if \"amazon\" in d.lower() or \"lokeshparab\" in d.lower():\n        print(\"\\nPossible dataset folder:\", d)\n        for f in os.listdir(os.path.join(base, d))[:50]:\n            print(\" \", f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:32:08.570101Z","iopub.execute_input":"2025-11-05T04:32:08.570265Z","iopub.status.idle":"2025-11-05T04:32:08.587844Z","shell.execute_reply.started":"2025-11-05T04:32:08.570249Z","shell.execute_reply":"2025-11-05T04:32:08.587106Z"}},"outputs":[{"name":"stdout","text":"Top-level folders in /kaggle/input:\n['amazon-products-dataset']\n\nPossible dataset folder: amazon-products-dataset\n  Gaming Consoles.csv\n  Car Electronics.csv\n  Janitorial and Sanitation Supplies.csv\n  All Electronics.csv\n  All Books.csv\n  Make-up.csv\n  Travel Accessories.csv\n  Indian Language Books.csv\n  Car and Bike Care.csv\n  Sunglasses.csv\n  Bags and Luggage.csv\n  Yoga.csv\n  Sportswear.csv\n  Fiction Books.csv\n  Exam Central.csv\n  Home Storage.csv\n  Toys Gifting Store.csv\n  All English.csv\n  Amazon-Products.csv\n  Air Conditioners.csv\n  Shoes.csv\n  Casual Shoes.csv\n  Baby Products.csv\n  Sports Collectibles.csv\n  Wallets.csv\n  Musical Instruments and Professional Audio.csv\n  Gold and Diamond Jewellery.csv\n  Nursing and Feeding.csv\n  Home Furnishing.csv\n  School Textbooks.csv\n  All Hindi.csv\n  Baby Bath Skin and Grooming.csv\n  Coffee Tea and Beverages.csv\n  Headphones.csv\n  Furniture.csv\n  Shirts.csv\n  Subscribe and Save.csv\n  Fitness Accessories.csv\n  Formal Shoes.csv\n  Cycling.csv\n  Western Wear.csv\n  Bedroom Linen.csv\n  Gaming Accessories.csv\n  Amazon Fashion.csv\n  Home Entertainment Systems.csv\n  Strollers and Prams.csv\n  Refurbished and Open Box.csv\n  Garden and Outdoors.csv\n  Diapers.csv\n  STEM Toys Store.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# âš™ï¸ Environment Setup â€” Hybrid Recommender (Kaggle)\n# ============================================================\n\n# Silence noisy dependency warnings\nimport warnings, os, sys\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"ğŸ Checking Python environment...\")\n!python3 --version\n!pip --version\n\n# ============================================================\n# âœ… Install exact compatible versions\n# ============================================================\n# - scikit-learn pinned below 1.6.0 to satisfy Kaggle preinstalls\n# - numpy pinned to 1.26.x (stable)\n# - xgboost & imbalanced-learn versions fully compatible with sklearn 1.5.x\n\n!pip install -q \\\n    scikit-learn==1.5.2 \\\n    numpy==1.26.4 \\\n    xgboost==2.0.3 \\\n    imbalanced-learn==0.12.3 \\\n    joblib==1.4.2 \\\n    pandas==2.2.2 \\\n    scipy==1.13.1\n\n# ============================================================\n# âœ… Confirm versions (so you know whatâ€™s running)\n# ============================================================\nimport sklearn, numpy, xgboost, imblearn, pandas, scipy, joblib\nprint(f\"scikit-learn: {sklearn.__version__}\")\nprint(f\"numpy:        {numpy.__version__}\")\nprint(f\"xgboost:      {xgboost.__version__}\")\nprint(f\"imblearn:     {imblearn.__version__}\")\nprint(f\"pandas:       {pandas.__version__}\")\nprint(f\"scipy:        {scipy.__version__}\")\nprint(f\"joblib:       {joblib.__version__}\")\n\nprint(\"\\nâœ… Environment ready! You can now safely run the next cells.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:32:57.212450Z","iopub.execute_input":"2025-11-05T04:32:57.212951Z","iopub.status.idle":"2025-11-05T04:33:19.805253Z","shell.execute_reply.started":"2025-11-05T04:32:57.212929Z","shell.execute_reply":"2025-11-05T04:33:19.804646Z"}},"outputs":[{"name":"stdout","text":"ğŸ Checking Python environment...\nPython 3.11.13\npip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mscikit-learn: 1.5.2\nnumpy:        1.26.4\nxgboost:      2.0.3\nimblearn:     0.12.3\npandas:       2.2.2\nscipy:        1.13.1\njoblib:       1.4.2\n\nâœ… Environment ready! You can now safely run the next cells.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install (Kaggle already has most packages, but safe)\n!pip install -q xgboost imbalanced-learn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nimport joblib\n\nrandom.seed(42)\nnp.random.seed(42)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:29.148553Z","iopub.execute_input":"2025-11-05T04:33:29.148946Z","iopub.status.idle":"2025-11-05T04:33:32.332821Z","shell.execute_reply.started":"2025-11-05T04:33:29.148925Z","shell.execute_reply":"2025-11-05T04:33:32.332001Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Update this to the folder name you saw in the inspect cell\nDATA_FOLDER = \"/kaggle/input/amazon-products-dataset\"  # <-- change if needed\n\n# list files\nfiles = os.listdir(DATA_FOLDER)\nprint(\"Files in dataset folder:\", files)\n\n# Try to load best candidate files\n# Common patterns: reviews.csv, products.csv, amazon_products.csv\ndf = None\nfor fname in files:\n    path = os.path.join(DATA_FOLDER, fname)\n    if fname.lower().endswith(\".csv\"):\n        print(\"Trying to read\", fname)\n        try:\n            tmp = pd.read_csv(path, nrows=5)\n            print(\"Sample columns:\", tmp.columns.tolist())\n            # Heuristic: if it contains 'reviewerID' or 'reviewText' it's reviews\n            if any(c.lower() in ['reviewerid','reviewer_id','user_id','user'] for c in tmp.columns):\n                # prefer a reviews file\n                df = pd.read_csv(path)\n                print(\"Loaded review file:\", fname, \"shape:\", df.shape)\n                break\n            # else if appears to be products with title/description\n            if any(c.lower() in ['title','product_title','name','description'] for c in tmp.columns):\n                # keep as product file (if no reviews file found, we will create pseudo interactions)\n                if df is None:\n                    df = pd.read_csv(path)\n                    print(\"Loaded product file (no reviews found yet):\", fname, \"shape:\", df.shape)\n        except Exception as e:\n            print(\"Failed to read\", fname, \":\", e)\n\nif df is None:\n    raise FileNotFoundError(\"Couldn't auto-detect a CSV file. Please upload or point DATA_FOLDER to the correct dataset.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:33:40.558479Z","iopub.execute_input":"2025-11-05T04:33:40.559358Z","iopub.status.idle":"2025-11-05T04:33:42.386768Z","shell.execute_reply.started":"2025-11-05T04:33:40.559283Z","shell.execute_reply":"2025-11-05T04:33:42.386099Z"}},"outputs":[{"name":"stdout","text":"Files in dataset folder: ['Gaming Consoles.csv', 'Car Electronics.csv', 'Janitorial and Sanitation Supplies.csv', 'All Electronics.csv', 'All Books.csv', 'Make-up.csv', 'Travel Accessories.csv', 'Indian Language Books.csv', 'Car and Bike Care.csv', 'Sunglasses.csv', 'Bags and Luggage.csv', 'Yoga.csv', 'Sportswear.csv', 'Fiction Books.csv', 'Exam Central.csv', 'Home Storage.csv', 'Toys Gifting Store.csv', 'All English.csv', 'Amazon-Products.csv', 'Air Conditioners.csv', 'Shoes.csv', 'Casual Shoes.csv', 'Baby Products.csv', 'Sports Collectibles.csv', 'Wallets.csv', 'Musical Instruments and Professional Audio.csv', 'Gold and Diamond Jewellery.csv', 'Nursing and Feeding.csv', 'Home Furnishing.csv', 'School Textbooks.csv', 'All Hindi.csv', 'Baby Bath Skin and Grooming.csv', 'Coffee Tea and Beverages.csv', 'Headphones.csv', 'Furniture.csv', 'Shirts.csv', 'Subscribe and Save.csv', 'Fitness Accessories.csv', 'Formal Shoes.csv', 'Cycling.csv', 'Western Wear.csv', 'Bedroom Linen.csv', 'Gaming Accessories.csv', 'Amazon Fashion.csv', 'Home Entertainment Systems.csv', 'Strollers and Prams.csv', 'Refurbished and Open Box.csv', 'Garden and Outdoors.csv', 'Diapers.csv', 'STEM Toys Store.csv', 'Kids Clothing.csv', 'All Car and Motorbike Products.csv', 'All Sports Fitness and Outdoors.csv', 'Film Songs.csv', 'All Home and Kitchen.csv', 'Speakers.csv', 'Beauty and Grooming.csv', 'Fashion and Silver Jewellery.csv', 'Televisions.csv', 'Cameras.csv', 'Home Improvement.csv', 'Blu-ray.csv', 'Kitchen and Home Appliances.csv', 'Health and Personal Care.csv', 'Sports Shoes.csv', 'Indoor Lighting.csv', 'Indian Classical.csv', 'Mens Fashion.csv', 'Kindle eBooks.csv', 'Snack Foods.csv', 'Kitchen Storage and Containers.csv', 'Strength Training.csv', 'International Music.csv', 'Video Games Deals.csv', 'Textbooks.csv', 'Fine Art.csv', 'All Grocery and Gourmet Foods.csv', 'Luxury Beauty.csv', 'Kids Shoes.csv', 'Jewellery.csv', 'Jeans.csv', 'Cricket.csv', 'Kids Fashion.csv', 'Baby Fashion.csv', 'Sewing and Craft Supplies.csv', 'Car Parts.csv', 'Pantry.csv', 'Car Accessories.csv', 'All Music.csv', 'Toys and Games.csv', 'Amazon Pharmacy.csv', 'Camping and Hiking.csv', 'Ethnic Wear.csv', 'Kids Watches.csv', 'Heating and Cooling Appliances.csv', 'Kitchen and Dining.csv', 'Personal Care Appliances.csv', 'Home Audio and Theater.csv', 'Security Cameras.csv', 'Fashion Sales and Deals.csv', 'Lab and Scientific.csv', 'Rucksacks.csv', 'Football.csv', 'Diet and Nutrition.csv', 'Lingerie and Nightwear.csv', 'School Bags.csv', 'T-shirts and Polos.csv', 'All Pet Supplies.csv', 'Value Bazaar.csv', 'Household Supplies.csv', 'Backpacks.csv', 'Fashion Sandals.csv', 'Camera Accessories.csv', 'Industrial and Scientific Supplies.csv', 'Motorbike Accessories and Parts.csv', 'All Appliances.csv', 'PC Games.csv', 'Running.csv', 'All Movies and TV Shows.csv', 'Dog supplies.csv', 'Entertainment Collectibles.csv', 'Washing Machines.csv', 'Cardio Equipment.csv', 'Refrigerators.csv', 'All Video Games.csv', 'Innerwear.csv', 'Suitcases and Trolley Bags.csv', 'Home Dcor.csv', 'Clothing.csv', 'Ballerinas.csv', 'All Exercise and Fitness.csv', 'Badminton.csv', 'Travel Duffles.csv', 'Handbags and Clutches.csv', 'Womens Fashion.csv', 'International Toy Store.csv', 'The Designer Boutique.csv', 'Watches.csv', 'Test Measure and Inspect.csv', 'Childrens Books.csv']\nTrying to read Gaming Consoles.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nLoaded product file (no reviews found yet): Gaming Consoles.csv shape: (0, 9)\nTrying to read Car Electronics.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Janitorial and Sanitation Supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Electronics.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Books.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Make-up.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Travel Accessories.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Indian Language Books.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Car and Bike Care.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Sunglasses.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Bags and Luggage.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Yoga.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Sportswear.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fiction Books.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Exam Central.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Storage.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Toys Gifting Store.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All English.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Amazon-Products.csv\nSample columns: ['Unnamed: 0', 'name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Air Conditioners.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Shoes.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Casual Shoes.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Baby Products.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Sports Collectibles.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Wallets.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Musical Instruments and Professional Audio.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Gold and Diamond Jewellery.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Nursing and Feeding.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Furnishing.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read School Textbooks.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Hindi.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Baby Bath Skin and Grooming.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Coffee Tea and Beverages.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Headphones.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Furniture.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Shirts.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Subscribe and Save.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fitness Accessories.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Formal Shoes.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Cycling.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Western Wear.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Bedroom Linen.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Gaming Accessories.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Amazon Fashion.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Entertainment Systems.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Strollers and Prams.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Refurbished and Open Box.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Garden and Outdoors.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Diapers.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read STEM Toys Store.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kids Clothing.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Car and Motorbike Products.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Sports Fitness and Outdoors.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Film Songs.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Home and Kitchen.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Speakers.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Beauty and Grooming.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fashion and Silver Jewellery.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Televisions.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Cameras.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Improvement.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Blu-ray.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kitchen and Home Appliances.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Health and Personal Care.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Sports Shoes.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Indoor Lighting.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Indian Classical.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Mens Fashion.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kindle eBooks.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Snack Foods.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kitchen Storage and Containers.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Strength Training.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read International Music.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Video Games Deals.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Textbooks.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fine Art.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Grocery and Gourmet Foods.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Luxury Beauty.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kids Shoes.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Jewellery.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Jeans.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Cricket.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kids Fashion.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Baby Fashion.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Sewing and Craft Supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Car Parts.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Pantry.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Car Accessories.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Music.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Toys and Games.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Amazon Pharmacy.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Camping and Hiking.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Ethnic Wear.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kids Watches.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Heating and Cooling Appliances.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Kitchen and Dining.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Personal Care Appliances.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Audio and Theater.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Security Cameras.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fashion Sales and Deals.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Lab and Scientific.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Rucksacks.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Football.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Diet and Nutrition.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Lingerie and Nightwear.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read School Bags.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read T-shirts and Polos.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Pet Supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Value Bazaar.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Household Supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Backpacks.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Fashion Sandals.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Camera Accessories.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Industrial and Scientific Supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Motorbike Accessories and Parts.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Appliances.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read PC Games.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Running.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Movies and TV Shows.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Dog supplies.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Entertainment Collectibles.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Washing Machines.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Cardio Equipment.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Refrigerators.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Video Games.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Innerwear.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Suitcases and Trolley Bags.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Home Dcor.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Clothing.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Ballerinas.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read All Exercise and Fitness.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Badminton.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Travel Duffles.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Handbags and Clutches.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Womens Fashion.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read International Toy Store.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read The Designer Boutique.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Watches.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Test Measure and Inspect.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nTrying to read Childrens Books.csv\nSample columns: ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Normalize column names (lowercase)\ndf.columns = [c.strip() for c in df.columns]\n\n# Heuristics to create interactions + product meta\nif any(c.lower() in ['reviewerid','reviewer_id','user'] for c in df.columns):\n    # assume review dataset: typical Amazon columns: reviewerID, asin, overall, reviewText, summary, title?\n    col_map = {c.lower(): c for c in df.columns}\n    # pick columns\n    user_col = next((c for c in df.columns if c.lower() in ['reviewerid','reviewer_id','user_id','user']), None)\n    item_col = next((c for c in df.columns if c.lower() in ['asin','productid','product_id','product']), None)\n    rating_col = next((c for c in df.columns if c.lower() in ['overall','rating','score']), None)\n    title_col = next((c for c in df.columns if c.lower() in ['title','product_title','name','headline']), None)\n    desc_col = next((c for c in df.columns if c.lower() in ['reviewtext','description','product_description','body']), None)\n\n    print(\"Detected columns -> user:\", user_col, \"item:\", item_col, \"rating:\", rating_col, \"title:\", title_col, \"desc:\", desc_col)\n    interactions = df[[user_col, item_col, rating_col] if rating_col else [user_col, item_col]].copy()\n    interactions.columns = ['user_id', 'product_id'] + (['rating'] if rating_col else ['rating'])\n    if rating_col is None:\n        interactions['rating'] = 1.0\n    else:\n        interactions['rating'] = interactions['rating'].astype(float)\n\n    # product meta: try to extract from df, else create placeholders\n    if title_col is None:\n        product_meta = interactions[['product_id']].drop_duplicates().reset_index(drop=True)\n        product_meta['title'] = product_meta['product_id'].astype(str)\n        product_meta['description'] = \"\"\n        product_meta['category'] = \"\"\n    else:\n        product_meta = df[[item_col, title_col]].drop_duplicates().rename(columns={item_col:'product_id', title_col:'title'})\n        if desc_col and desc_col in df.columns:\n            product_meta = product_meta.merge(df[[item_col, desc_col]].drop_duplicates().rename(columns={item_col:'product_id', desc_col:'description'}), on='product_id', how='left')\n        else:\n            product_meta['description'] = \"\"\n        if 'category' not in product_meta.columns:\n            product_meta['category'] = \"\"\nelse:\n    # product-only file: create synthetic interactions (lightweight)\n    print(\"Dataset appears to be product metadata only. Creating synthetic demo interactions.\")\n    product_meta = df.copy()\n    if 'product_id' not in product_meta.columns:\n        # choose a column to be product id (first column)\n        product_meta = product_meta.rename(columns={product_meta.columns[0]:'product_id'})\n    if 'title' not in product_meta.columns:\n        product_meta['title'] = product_meta['product_id'].astype(str)\n    if 'description' not in product_meta.columns:\n        product_meta['description'] = product_meta['title'].astype(str)\n    # synthesize interactions: random users picking products\n    n_users = 1000\n    interactions = []\n    products = product_meta['product_id'].unique().tolist()\n    for u in range(n_users):\n        picked = random.sample(products, k=min(20, len(products)))\n        for p in picked:\n            interactions.append({'user_id': f\"U{u}\", 'product_id': p, 'rating': 1.0})\n    interactions = pd.DataFrame(interactions)\n\n# quick shapes and samples\nprint(\"Interactions shape:\", interactions.shape)\nprint(\"Product meta shape:\", product_meta.shape)\ninteractions.head(), product_meta.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:34:33.022695Z","iopub.execute_input":"2025-11-05T04:34:33.023276Z","iopub.status.idle":"2025-11-05T04:34:33.045473Z","shell.execute_reply.started":"2025-11-05T04:34:33.023253Z","shell.execute_reply":"2025-11-05T04:34:33.044863Z"}},"outputs":[{"name":"stdout","text":"Dataset appears to be product metadata only. Creating synthetic demo interactions.\nInteractions shape: (0, 0)\nProduct meta shape: (0, 11)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(Empty DataFrame\n Columns: []\n Index: [],\n Empty DataFrame\n Columns: [product_id, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price, title, description]\n Index: [])"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# ğŸ§  FIXED: Auto-load Amazon dataset and create interactions\n# Works for \"lokeshparab/amazon-products-dataset\" and similar\n# ============================================================\n\nimport pandas as pd, os, random\n\n# Point to your Kaggle dataset folder\nDATA_FOLDER = \"/kaggle/input/amazon-products-dataset\"  # âœ… adjust if name differs\nprint(\"Files in dataset folder:\", os.listdir(DATA_FOLDER))\n\n# Try to read any CSV file in that folder\ndf = None\nfor fname in os.listdir(DATA_FOLDER):\n    if fname.lower().endswith(\".csv\"):\n        path = os.path.join(DATA_FOLDER, fname)\n        try:\n            tmp = pd.read_csv(path, nrows=5)\n            print(f\"âœ… Loaded sample of {fname}\")\n            print(tmp.columns.tolist())\n            df = pd.read_csv(path)\n            break\n        except Exception as e:\n            print(f\"âš ï¸ Could not read {fname}: {e}\")\n\nif df is None:\n    raise ValueError(\"No readable CSV file found in dataset folder.\")\n\nprint(\"Full dataset shape:\", df.shape)\n\n# ---------------------------------------------\n# ğŸ”§ Step 2 â€” Auto-detect and rename columns\n# ---------------------------------------------\nrename_map = {}\n\nfor c in df.columns:\n    cl = c.lower()\n    if cl in ['reviewerid','reviewer_id','user','userid','user_id','customerid','customer_id']:\n        rename_map[c] = 'user_id'\n    if cl in ['asin','product','productid','product_id','item','itemid','item_id','sku']:\n        rename_map[c] = 'product_id'\n    if cl in ['rating','overall','score','stars']:\n        rename_map[c] = 'rating'\n    if cl in ['title','product_title','name','summary','headline']:\n        rename_map[c] = 'title'\n    if cl in ['reviewtext','description','product_description','body']:\n        rename_map[c] = 'description'\n    if 'category' in cl:\n        rename_map[c] = 'category'\n\ndf = df.rename(columns=rename_map)\nprint(\"Renamed columns:\", rename_map)\nprint(\"Columns after renaming:\", df.columns.tolist())\n\n# ---------------------------------------------\n# ğŸ”§ Step 3 â€” Build interactions + product meta\n# ---------------------------------------------\n# If dataset has users & products -> use real interactions\nif 'user_id' in df.columns and 'product_id' in df.columns:\n    print(\"âœ… Detected review-style dataset (real interactions).\")\n    interactions = df[['user_id', 'product_id']].copy()\n    if 'rating' in df.columns:\n        interactions['rating'] = df['rating'].astype(float)\n    else:\n        interactions['rating'] = 1.0\n\n    # product meta\n    meta_cols = ['product_id']\n    for c in ['title','description','category']:\n        if c in df.columns:\n            meta_cols.append(c)\n    product_meta = df[meta_cols].drop_duplicates('product_id').reset_index(drop=True)\n\n# If only product info exists -> create synthetic interactions\nelse:\n    print(\"âš ï¸ No user_id or product_id columns found â€” creating synthetic demo interactions.\")\n    if 'product_id' not in df.columns:\n        df = df.rename(columns={df.columns[0]: 'product_id'})\n    if 'title' not in df.columns:\n        df['title'] = df['product_id'].astype(str)\n    if 'description' not in df.columns:\n        df['description'] = df['title'].astype(str)\n    product_meta = df[['product_id','title','description']].drop_duplicates('product_id')\n    \n    n_users = 500\n    interactions = []\n    products = product_meta['product_id'].tolist()\n    for u in range(n_users):\n        picked = random.sample(products, k=min(15, len(products)))\n        for p in picked:\n            interactions.append({'user_id': f'U{u}', 'product_id': p, 'rating': 1.0})\n    interactions = pd.DataFrame(interactions)\n\nprint(\"\\nâœ… interactions shape:\", interactions.shape)\nprint(\"âœ… product_meta shape:\", product_meta.shape)\nprint(\"\\nSample of interactions:\")\ndisplay(interactions.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:34:38.992425Z","iopub.execute_input":"2025-11-05T04:34:38.993056Z","iopub.status.idle":"2025-11-05T04:34:39.019219Z","shell.execute_reply.started":"2025-11-05T04:34:38.993021Z","shell.execute_reply":"2025-11-05T04:34:39.018521Z"}},"outputs":[{"name":"stdout","text":"Files in dataset folder: ['Gaming Consoles.csv', 'Car Electronics.csv', 'Janitorial and Sanitation Supplies.csv', 'All Electronics.csv', 'All Books.csv', 'Make-up.csv', 'Travel Accessories.csv', 'Indian Language Books.csv', 'Car and Bike Care.csv', 'Sunglasses.csv', 'Bags and Luggage.csv', 'Yoga.csv', 'Sportswear.csv', 'Fiction Books.csv', 'Exam Central.csv', 'Home Storage.csv', 'Toys Gifting Store.csv', 'All English.csv', 'Amazon-Products.csv', 'Air Conditioners.csv', 'Shoes.csv', 'Casual Shoes.csv', 'Baby Products.csv', 'Sports Collectibles.csv', 'Wallets.csv', 'Musical Instruments and Professional Audio.csv', 'Gold and Diamond Jewellery.csv', 'Nursing and Feeding.csv', 'Home Furnishing.csv', 'School Textbooks.csv', 'All Hindi.csv', 'Baby Bath Skin and Grooming.csv', 'Coffee Tea and Beverages.csv', 'Headphones.csv', 'Furniture.csv', 'Shirts.csv', 'Subscribe and Save.csv', 'Fitness Accessories.csv', 'Formal Shoes.csv', 'Cycling.csv', 'Western Wear.csv', 'Bedroom Linen.csv', 'Gaming Accessories.csv', 'Amazon Fashion.csv', 'Home Entertainment Systems.csv', 'Strollers and Prams.csv', 'Refurbished and Open Box.csv', 'Garden and Outdoors.csv', 'Diapers.csv', 'STEM Toys Store.csv', 'Kids Clothing.csv', 'All Car and Motorbike Products.csv', 'All Sports Fitness and Outdoors.csv', 'Film Songs.csv', 'All Home and Kitchen.csv', 'Speakers.csv', 'Beauty and Grooming.csv', 'Fashion and Silver Jewellery.csv', 'Televisions.csv', 'Cameras.csv', 'Home Improvement.csv', 'Blu-ray.csv', 'Kitchen and Home Appliances.csv', 'Health and Personal Care.csv', 'Sports Shoes.csv', 'Indoor Lighting.csv', 'Indian Classical.csv', 'Mens Fashion.csv', 'Kindle eBooks.csv', 'Snack Foods.csv', 'Kitchen Storage and Containers.csv', 'Strength Training.csv', 'International Music.csv', 'Video Games Deals.csv', 'Textbooks.csv', 'Fine Art.csv', 'All Grocery and Gourmet Foods.csv', 'Luxury Beauty.csv', 'Kids Shoes.csv', 'Jewellery.csv', 'Jeans.csv', 'Cricket.csv', 'Kids Fashion.csv', 'Baby Fashion.csv', 'Sewing and Craft Supplies.csv', 'Car Parts.csv', 'Pantry.csv', 'Car Accessories.csv', 'All Music.csv', 'Toys and Games.csv', 'Amazon Pharmacy.csv', 'Camping and Hiking.csv', 'Ethnic Wear.csv', 'Kids Watches.csv', 'Heating and Cooling Appliances.csv', 'Kitchen and Dining.csv', 'Personal Care Appliances.csv', 'Home Audio and Theater.csv', 'Security Cameras.csv', 'Fashion Sales and Deals.csv', 'Lab and Scientific.csv', 'Rucksacks.csv', 'Football.csv', 'Diet and Nutrition.csv', 'Lingerie and Nightwear.csv', 'School Bags.csv', 'T-shirts and Polos.csv', 'All Pet Supplies.csv', 'Value Bazaar.csv', 'Household Supplies.csv', 'Backpacks.csv', 'Fashion Sandals.csv', 'Camera Accessories.csv', 'Industrial and Scientific Supplies.csv', 'Motorbike Accessories and Parts.csv', 'All Appliances.csv', 'PC Games.csv', 'Running.csv', 'All Movies and TV Shows.csv', 'Dog supplies.csv', 'Entertainment Collectibles.csv', 'Washing Machines.csv', 'Cardio Equipment.csv', 'Refrigerators.csv', 'All Video Games.csv', 'Innerwear.csv', 'Suitcases and Trolley Bags.csv', 'Home Dcor.csv', 'Clothing.csv', 'Ballerinas.csv', 'All Exercise and Fitness.csv', 'Badminton.csv', 'Travel Duffles.csv', 'Handbags and Clutches.csv', 'Womens Fashion.csv', 'International Toy Store.csv', 'The Designer Boutique.csv', 'Watches.csv', 'Test Measure and Inspect.csv', 'Childrens Books.csv']\nâœ… Loaded sample of Gaming Consoles.csv\n['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nFull dataset shape: (0, 9)\nRenamed columns: {'name': 'title', 'main_category': 'category', 'sub_category': 'category'}\nColumns after renaming: ['title', 'category', 'category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nâš ï¸ No user_id or product_id columns found â€” creating synthetic demo interactions.\n\nâœ… interactions shape: (0, 0)\nâœ… product_meta shape: (0, 3)\n\nSample of interactions:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: []\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Robust loader: pick a good CSV, create product_meta + interactions (falls back to synthetic)\nimport os, pandas as pd, random\nfrom scipy.sparse import csr_matrix\n\nDATA_FOLDER = \"/kaggle/input/amazon-products-dataset\"  # adjust if needed\n\n# 1) Inspect CSV files and sizes / row counts\ncsv_files = [f for f in os.listdir(DATA_FOLDER) if f.lower().endswith('.csv')]\ninfo = []\nprint(\"Found CSV files (name, bytes, rows sample):\\n\")\nfor f in csv_files:\n    path = os.path.join(DATA_FOLDER, f)\n    try:\n        size = os.path.getsize(path)\n        # quick check rows (read small nrows to avoid heavy io)\n        tmp = pd.read_csv(path, nrows=5)\n        rows_sample = len(tmp)\n        info.append((f, size, rows_sample))\n        print(f\" - {f:60s}  {size:10d} bytes  sample_rows={rows_sample}  cols={list(tmp.columns)[:6]}\")\n    except Exception as e:\n        print(f\" - {f:60s}  FAILED to read: {e}\")\n\n# 2) choose the largest non-empty CSV by bytes (skips very small files)\ncandidates = [t for t in info if t[1] > 200 and t[2] > 0]  # threshold: >200 bytes and sample rows >0\nif not candidates:\n    # relax threshold if nothing found\n    candidates = [t for t in info if t[2] > 0]\nif not candidates:\n    raise FileNotFoundError(\"No non-empty CSV files found in dataset folder. Pick a different dataset or upload files.\")\n\n# Sort candidates by file size descending and pick top\ncandidates.sort(key=lambda x: x[1], reverse=True)\nchosen_fname = candidates[0][0]\nprint(f\"\\nâœ… Choosing file: {chosen_fname}\\n\")\nchosen_path = os.path.join(DATA_FOLDER, chosen_fname)\n\n# 3) load the chosen CSV (full read)\ndf = pd.read_csv(chosen_path)\nprint(\"Loaded shape:\", df.shape)\nprint(\"Columns (sample):\", df.columns.tolist()[:30])\n\n# 4) standardize and detect columns\nrename_map = {}\nfor c in df.columns:\n    cl = c.strip().lower()\n    if cl in ['reviewerid','reviewer_id','user','userid','user_id','customerid','customer_id']:\n        rename_map[c] = 'user_id'\n    if cl in ['asin','product','productid','product_id','item','itemid','item_id','sku','id','productId'.lower()]:\n        # prefer asin/product-like names\n        rename_map[c] = 'product_id'\n    if cl in ['rating','overall','score','stars']:\n        rename_map[c] = 'rating'\n    if cl in ['title','product_title','name','summary','headline']:\n        rename_map[c] = 'title'\n    if cl in ['reviewtext','description','product_description','body','desc']:\n        rename_map[c] = 'description'\n    if 'category' in cl:\n        # if there are multiple category columns we'll keep both temporarily\n        rename_map[c] = 'category'\n\ndf = df.rename(columns=rename_map)\nprint(\"After renaming, columns include:\", list(df.columns)[:30])\n\n# 5) create product_meta\n# Try to gather product_id/title/description/category. If not present, synthesize product_id.\nproduct_meta = None\nif 'product_id' in df.columns:\n    # Try to collect title/description/category if present\n    meta_cols = ['product_id']\n    for c in ['title','description','category']:\n        if c in df.columns:\n            meta_cols.append(c)\n    # drop duplicates on product_id\n    product_meta = df[meta_cols].drop_duplicates('product_id').reset_index(drop=True)\nelse:\n    # No product id column; try to construct one from 'link' or index\n    if 'link' in df.columns:\n        df['product_id'] = df['link'].astype(str)\n    else:\n        df['product_id'] = df.index.astype(str).map(lambda x: f\"P{x}\")\n    # ensure title and description exist\n    if 'title' not in df.columns:\n        df['title'] = df.iloc[:,0].astype(str)\n    if 'description' not in df.columns:\n        df['description'] = df['title'].astype(str)\n    product_meta = df[['product_id','title','description']].drop_duplicates('product_id').reset_index(drop=True)\n\nprint(\"product_meta sample shape:\", product_meta.shape)\ndisplay(product_meta.head())\n\n# 6) Create interactions if user info present; otherwise synthesize interactions\nif 'user_id' in df.columns and 'product_id' in df.columns:\n    print(\"Detected user interactions in file. Building interactions from real data.\")\n    interactions = df[['user_id','product_id']].copy()\n    interactions['rating'] = df['rating'].astype(float) if 'rating' in df.columns else 1.0\nelse:\n    # synthesize interactions â€” ensure we have some products to sample\n    n_products = product_meta.shape[0]\n    if n_products == 0:\n        raise RuntimeError(\"No products found to create interactions from. Try a different file in the dataset.\")\n    print(\"No user info detected. Creating synthetic interactions.\")\n    n_users = 500\n    interactions_list = []\n    all_products = product_meta['product_id'].tolist()\n    # if dataset is tiny, reduce users; ensure each user has min 5 interactions\n    interactions_per_user = min(25, max(5, n_products//10))\n    for u in range(n_users):\n        chosen = random.sample(all_products, k=min(interactions_per_user, len(all_products)))\n        for p in chosen:\n            interactions_list.append({'user_id': f\"U{u}\", 'product_id': p, 'rating': 1.0})\n    interactions = pd.DataFrame(interactions_list)\n\nprint(\"interactions shape before pruning:\", interactions.shape)\ndisplay(interactions.head())\n\n# 7) Prune rare users/items (optional)\nmin_user_interactions = 3\nmin_item_interactions = 3\nuser_counts = interactions['user_id'].value_counts()\nvalid_users = user_counts[user_counts >= min_user_interactions].index\nitem_counts = interactions['product_id'].value_counts()\nvalid_items = item_counts[item_counts >= min_item_interactions].index\n\ninteractions = interactions[interactions['user_id'].isin(valid_users) & interactions['product_id'].isin(valid_items)].copy()\nprint(\"interactions shape after pruning:\", interactions.shape)\n\n# 8) Ensure product_meta only contains products present in interactions\nproduct_meta = product_meta[product_meta['product_id'].isin(interactions['product_id'].unique())].reset_index(drop=True)\nprint(\"product_meta shape after filtering:\", product_meta.shape)\n\n# 9) Build mappings and sparse matrix\nuser_ids = interactions['user_id'].unique().tolist()\nitem_ids = interactions['product_id'].unique().tolist()\nuser_to_index = {u:i for i,u in enumerate(user_ids)}\nindex_to_user = {i:u for u,i in user_to_index.items()}\nitem_to_index = {p:i for i,p in enumerate(item_ids)}\nindex_to_item = {i:p for p,i in item_to_index.items()}\n\nrows = interactions['user_id'].map(user_to_index)\ncols = interactions['product_id'].map(item_to_index)\ndata = interactions['rating'].astype(float)\nuser_item_csr = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids)))\n\nprint(\"\\nâœ… Final diagnostics:\")\nprint(\" - chosen CSV:\", chosen_fname)\nprint(\" - interactions (users x items):\", user_item_csr.shape)\nprint(\" - unique users:\", len(user_ids), \"unique items:\", len(item_ids))\nprint(\" - sample users:\", user_ids[:3])\nprint(\" - sample items:\", item_ids[:3])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:34:46.189220Z","iopub.execute_input":"2025-11-05T04:34:46.189521Z","iopub.status.idle":"2025-11-05T04:34:52.862043Z","shell.execute_reply.started":"2025-11-05T04:34:46.189499Z","shell.execute_reply":"2025-11-05T04:34:52.861282Z"}},"outputs":[{"name":"stdout","text":"Found CSV files (name, bytes, rows sample):\n\n - Gaming Consoles.csv                                                   94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Car Electronics.csv                                               378730 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Janitorial and Sanitation Supplies.csv                            355062 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Electronics.csv                                              3691715 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Books.csv                                                         94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Make-up.csv                                                       891819 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Travel Accessories.csv                                            662838 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Indian Language Books.csv                                             94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Car and Bike Care.csv                                             341209 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Sunglasses.csv                                                    463184 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Bags and Luggage.csv                                             6563675 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Yoga.csv                                                          357865 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Sportswear.csv                                                   2129639 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fiction Books.csv                                                     94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Exam Central.csv                                                      94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Storage.csv                                                  451264 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Toys Gifting Store.csv                                              8573 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All English.csv                                                       94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Amazon-Products.csv                                            188603963 bytes  sample_rows=5  cols=['Unnamed: 0', 'name', 'main_category', 'sub_category', 'image', 'link']\n - Air Conditioners.csv                                              247753 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Shoes.csv                                                         560965 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Casual Shoes.csv                                                 5736321 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Baby Products.csv                                                 383790 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Sports Collectibles.csv                                               94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Wallets.csv                                                       663393 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Musical Instruments and Professional Audio.csv                    403018 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Gold and Diamond Jewellery.csv                                   6058375 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Nursing and Feeding.csv                                           385563 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Furnishing.csv                                               521791 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - School Textbooks.csv                                                  94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Hindi.csv                                                         94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Baby Bath Skin and Grooming.csv                                   513455 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Coffee Tea and Beverages.csv                                      459639 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Headphones.csv                                                   3627183 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Furniture.csv                                                     485076 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Shirts.csv                                                       5751549 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Subscribe and Save.csv                                                94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fitness Accessories.csv                                           470308 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Formal Shoes.csv                                                 5785257 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Cycling.csv                                                       390473 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Western Wear.csv                                                 6114125 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Bedroom Linen.csv                                                 451595 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Gaming Accessories.csv                                                94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Amazon Fashion.csv                                                793483 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Entertainment Systems.csv                                   3724917 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Strollers and Prams.csv                                           225954 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Refurbished and Open Box.csv                                        8161 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Garden and Outdoors.csv                                           392254 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Diapers.csv                                                       389043 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - STEM Toys Store.csv                                                18136 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kids Clothing.csv                                                 673575 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Car and Motorbike Products.csv                                474820 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Sports Fitness and Outdoors.csv                               444480 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Film Songs.csv                                                        94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Home and Kitchen.csv                                          451431 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Speakers.csv                                                     3571993 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Beauty and Grooming.csv                                           681311 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fashion and Silver Jewellery.csv                                 7045316 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Televisions.csv                                                   369823 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Cameras.csv                                                      3596052 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Improvement.csv                                              379822 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Blu-ray.csv                                                           94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kitchen and Home Appliances.csv                                  3545354 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Health and Personal Care.csv                                      399638 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Sports Shoes.csv                                                 5892581 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Indoor Lighting.csv                                               449612 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Indian Classical.csv                                                  94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Mens Fashion.csv                                                 6193234 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kindle eBooks.csv                                                     94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Snack Foods.csv                                                   363955 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kitchen Storage and Containers.csv                                385771 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Strength Training.csv                                             403990 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - International Music.csv                                               94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Video Games Deals.csv                                                 94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Textbooks.csv                                                         94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fine Art.csv                                                          94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Grocery and Gourmet Foods.csv                                 336128 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Luxury Beauty.csv                                                 293659 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kids Shoes.csv                                                    682969 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Jewellery.csv                                                    6730217 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Jeans.csv                                                        5689779 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Cricket.csv                                                       388540 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kids Fashion.csv                                                  918331 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Baby Fashion.csv                                                  732098 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Sewing and Craft Supplies.csv                                     467899 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Car Parts.csv                                                     423697 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Pantry.csv                                                            94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Car Accessories.csv                                               512713 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Music.csv                                                         94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Toys and Games.csv                                                339167 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Amazon Pharmacy.csv                                                   94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Camping and Hiking.csv                                            286071 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Ethnic Wear.csv                                                  6344238 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kids Watches.csv                                                  712544 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Heating and Cooling Appliances.csv                               3489840 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Kitchen and Dining.csv                                            440515 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Personal Care Appliances.csv                                      449961 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Audio and Theater.csv                                        142420 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Security Cameras.csv                                             3619595 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fashion Sales and Deals.csv                                        13025 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Lab and Scientific.csv                                            397455 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Rucksacks.csv                                                     826483 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Football.csv                                                      390473 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Diet and Nutrition.csv                                            409520 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Lingerie and Nightwear.csv                                       6609118 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - School Bags.csv                                                   782792 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - T-shirts and Polos.csv                                           6145385 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Pet Supplies.csv                                              226297 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Value Bazaar.csv                                                   21482 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Household Supplies.csv                                            384315 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Backpacks.csv                                                     697757 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Fashion Sandals.csv                                               665372 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Camera Accessories.csv                                           3681698 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Industrial and Scientific Supplies.csv                            238974 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Motorbike Accessories and Parts.csv                               446241 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Appliances.csv                                               3490779 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - PC Games.csv                                                          94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Running.csv                                                       285922 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Movies and TV Shows.csv                                           94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Dog supplies.csv                                                  350208 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Entertainment Collectibles.csv                                        94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Washing Machines.csv                                              510337 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Cardio Equipment.csv                                               86997 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Refrigerators.csv                                                 747127 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Video Games.csv                                                   94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Innerwear.csv                                                    6085573 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Suitcases and Trolley Bags.csv                                    417911 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Home Dcor.csv                                                     461369 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Clothing.csv                                                     6402549 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Ballerinas.csv                                                    433709 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - All Exercise and Fitness.csv                                      410062 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Badminton.csv                                                     395399 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Travel Duffles.csv                                                406106 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Handbags and Clutches.csv                                        6260299 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Womens Fashion.csv                                                698356 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - International Toy Store.csv                                         8329 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - The Designer Boutique.csv                                         556626 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Watches.csv                                                      6151619 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Test Measure and Inspect.csv                                      530554 bytes  sample_rows=5  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n - Childrens Books.csv                                                   94 bytes  sample_rows=0  cols=['name', 'main_category', 'sub_category', 'image', 'link', 'ratings']\n\nâœ… Choosing file: Amazon-Products.csv\n\nLoaded shape: (551585, 10)\nColumns (sample): ['Unnamed: 0', 'name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nAfter renaming, columns include: ['Unnamed: 0', 'title', 'category', 'category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nproduct_meta sample shape: (551585, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                          product_id  \\\n0  https://www.amazon.in/Lloyd-Inverter-Convertib...   \n1  https://www.amazon.in/LG-Convertible-Anti-Viru...   \n2  https://www.amazon.in/LG-Inverter-Convertible-...   \n3  https://www.amazon.in/LG-Convertible-Anti-Viru...   \n4  https://www.amazon.in/Carrier-Inverter-Split-C...   \n\n                                               title  \\\n0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...   \n1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...   \n2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...   \n3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...   \n4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...   \n\n                                         description  \n0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...  \n1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...  \n2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...  \n3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...  \n4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>title</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.amazon.in/Lloyd-Inverter-Convertib...</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.amazon.in/LG-Convertible-Anti-Viru...</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.amazon.in/LG-Inverter-Convertible-...</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.amazon.in/LG-Convertible-Anti-Viru...</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.amazon.in/Carrier-Inverter-Split-C...</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"No user info detected. Creating synthetic interactions.\ninteractions shape before pruning: (12500, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  user_id                                         product_id  rating\n0      U0  https://www.amazon.in/ORENAME-Invisible-Silico...     1.0\n1      U0  https://www.amazon.in/Shrey-SHREY-Rucksack-Bag...     1.0\n2      U0  https://www.amazon.in/MERSODA%C2%AE-Stylish-Co...     1.0\n3      U0  https://www.amazon.in/AmazonBasics-Lightning-C...     1.0\n4      U0  https://www.amazon.in/Zebronics-Zeb-Symphony-B...     1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/ORENAME-Invisible-Silico...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/Shrey-SHREY-Rucksack-Bag...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/MERSODA%C2%AE-Stylish-Co...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/AmazonBasics-Lightning-C...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/Zebronics-Zeb-Symphony-B...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"interactions shape after pruning: (0, 3)\nproduct_meta shape after filtering: (0, 3)\n\nâœ… Final diagnostics:\n - chosen CSV: Amazon-Products.csv\n - interactions (users x items): (0, 0)\n - unique users: 0 unique items: 0\n - sample users: []\n - sample items: []\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================\n# ğŸ§  Train/Test Split (per-user holdout)\n# ============================================================\n\nimport random\nrandom.seed(42)\n\ndef train_test_holdout(df, user_col='user_id', item_col='product_id'):\n    \"\"\"Splits data so that each user has 1 item held out for testing.\"\"\"\n    train_idx, test_idx = [], []\n    for user, group in df.groupby(user_col):\n        idxs = group.index.tolist()\n        if len(idxs) <= 1:\n            train_idx.extend(idxs)\n        else:\n            holdout = random.choice(idxs)\n            test_idx.append(holdout)\n            train_idx.extend([i for i in idxs if i != holdout])\n    train_df = df.loc[train_idx].reset_index(drop=True)\n    test_df = df.loc[test_idx].reset_index(drop=True)\n    return train_df, test_df\n\ntrain_df, test_df = train_test_holdout(interactions)\n\nprint(\"Train interactions:\", train_df.shape[0])\nprint(\"Test interactions:\", test_df.shape[0])\n\n# Rebuild sparse train matrix\nfrom scipy.sparse import csr_matrix\nrows = train_df['user_id'].map(user_to_index)\ncols = train_df['product_id'].map(item_to_index)\ndata = train_df['rating'].astype(float)\ntrain_csr = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids)))\nprint(\"Train CSR shape:\", train_csr.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:37:45.777372Z","iopub.execute_input":"2025-11-05T04:37:45.777997Z","iopub.status.idle":"2025-11-05T04:37:45.788893Z","shell.execute_reply.started":"2025-11-05T04:37:45.777972Z","shell.execute_reply":"2025-11-05T04:37:45.788107Z"}},"outputs":[{"name":"stdout","text":"Train interactions: 0\nTest interactions: 0\nTrain CSR shape: (0, 0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Single cell: load products CSV -> build product_meta -> TF-IDF -> SVD -> Annoy neighbors\n# Paste and run in Kaggle/Jupyter.\n\n# If annoy is not installed in your kernel, uncomment:\n# !pip install annoy\n\nimport os, re, time, joblib, math\nimport numpy as np, pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom annoy import AnnoyIndex\n\nstart = time.time()\nprint(\"Start pipeline...\")\n\n# 1) Find a likely Amazon products CSV under /kaggle/input\ndef find_product_csv(input_root=\"/kaggle/input\"):\n    if not os.path.exists(input_root):\n        return None\n    for root, dirs, files in os.walk(input_root):\n        for f in files:\n            fn = f.lower()\n            if fn.endswith(\".csv\") and (\"amazon\" in fn or \"product\" in fn or \"products\" in fn):\n                # prefer files with 'amazon' and 'product' in name\n                return os.path.join(root, f)\n    return None\n\ncsv_path = find_product_csv()\nif csv_path is None:\n    raise FileNotFoundError(\"No product CSV found under /kaggle/input. Upload your dataset or set `csv_path` manually.\")\n\nprint(\"Loading CSV:\", csv_path)\n# read with robust settings\ndf = pd.read_csv(csv_path, dtype=str, encoding='utf-8', engine='c')\nprint(\"Raw dataframe shape:\", df.shape)\ndisplay(df.head(3))\n\n# 2) Build product_meta: pick sensible columns and ensure product_id + title exist\ndf_columns = [c.strip() for c in df.columns]\ndf.columns = df_columns\n\n# attempt to find ID column\nid_candidates = [c for c in df.columns if c.lower() in ('product_id','id','asin','sku')]\nprod_id_col = id_candidates[0] if id_candidates else None\nif prod_id_col is None:\n    # create synthetic id\n    df.insert(0, 'product_id', [f\"P{i}\" for i in range(len(df))])\n    prod_id_col = 'product_id'\n\n# attempt to find title/name column\ntitle_candidates = [c for c in df.columns if c.lower() in ('title','name','product_name','product title')]\ntitle_col = title_candidates[0] if title_candidates else None\nif title_col is None:\n    # fallback to first object-type column that is not url/image\n    obj_cols = [c for c in df.columns if df[c].dtype == 'object']\n    def looks_like_url(col):\n        sample = df[col].dropna().astype(str).head(8).tolist()\n        if not sample: return False\n        return sum(1 for s in sample if re.search(r'https?://|www\\.|\\.jpg|\\.png|/dp/|amazon\\.', s.lower())) >= 1\n    obj_cols = [c for c in obj_cols if not looks_like_url(c)]\n    title_col = obj_cols[0] if obj_cols else prod_id_col\n\n# create product_meta with columns: product_id, title plus everything else kept\nproduct_meta = df.copy()\nproduct_meta['product_id'] = product_meta[prod_id_col].astype(str)\nproduct_meta['title'] = product_meta[title_col].astype(str) if title_col in product_meta.columns else product_meta['product_id'].astype(str)\n\n# 3) Build a robust text field from available textual columns\npreferred_text_cols = ['title','name','product_name','description','desc','category','categories','brand','features','specs']\navailable = [c for c in preferred_text_cols if c in product_meta.columns]\n# also add other object dtype columns (excluding urls/images)\nif not available:\n    obj_cols = [c for c in product_meta.columns if product_meta[c].dtype == 'object']\n    def is_url_col(col):\n        sample = product_meta[col].dropna().astype(str).head(6).tolist()\n        if not sample: return False\n        return sum(1 for s in sample if re.search(r'https?://|www\\.|\\.jpg|\\.png|/dp/|amazon\\.', s.lower())) >= 1\n    obj_cols = [c for c in obj_cols if not is_url_col(c)]\n    available = obj_cols[:3]  # pick a few\n\nif 'title' not in available:\n    available = ['title'] + available\n\nprint(\"Text source columns used:\", available)\n\ndef build_text(row):\n    parts = []\n    for c in available:\n        if c in row and pd.notna(row[c]):\n            s = str(row[c]).strip()\n            if s and not re.match(r'^\\s*https?://', s):\n                parts.append(s)\n    if parts:\n        return \" \".join(parts)\n    # fallback to product_id\n    if pd.notna(row['product_id']):\n        return str(row['product_id'])\n    return \"\"\n\nproduct_meta['text'] = product_meta.apply(build_text, axis=1)\n\n# 4) Coerce to safe strings (handle lists/dicts), drop empties\ndef to_safe_text(x):\n    if x is None:\n        return \"\"\n    if isinstance(x, (list, tuple, set)):\n        return \" \".join(str(i) for i in x if i is not None)\n    if isinstance(x, dict):\n        return \" \".join(f\"{k} {v}\" for k,v in x.items())\n    return str(x)\nproduct_meta['text'] = product_meta['text'].apply(to_safe_text).map(lambda s: s.strip())\nproduct_meta.loc[product_meta['text'] == \"\", 'text'] = pd.NA\n\nprint(\"Non-empty text rows (before fallback):\", int(product_meta['text'].notna().sum()), \"/\", len(product_meta))\n\n# If too few rows have text, create fallback tokens from title/product_id\nmin_usable = max(1, int(0.01 * len(product_meta)))  # require at least 1% usable docs\nif int(product_meta['text'].notna().sum()) < min_usable:\n    print(\"Too few usable text rows; creating fallback text from title/product_id...\")\n    def fallback_text(row):\n        t = row.get('title') or \"\"\n        if pd.notna(t) and str(t).strip():\n            toks = re.findall(r'\\b[a-zA-Z0-9]{2,}\\b', str(t))\n            return \" \".join(toks[:6]) if toks else (\"product_\" + str(row.name))\n        if pd.notna(row.get('product_id')):\n            return str(row.get('product_id'))\n        return \"product_\" + str(row.name)\n    mask = product_meta['text'].isna()\n    product_meta.loc[mask, 'text'] = product_meta.loc[mask].apply(fallback_text, axis=1)\n\nproduct_meta = product_meta.dropna(subset=['text']).reset_index(drop=True)\nprint(\"Final product_meta shape:\", product_meta.shape)\ndisplay(product_meta[['product_id','title','text']].head(5))\n\n# 5) Clean text for TF-IDF\ndef clean_text(s):\n    s = str(s).lower()\n    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\nproduct_meta['text_clean'] = product_meta['text'].astype(str).map(clean_text)\nproduct_meta['text_len'] = product_meta['text_clean'].str.len().fillna(0).astype(int)\nprint(\"Average cleaned text length:\", product_meta['text_len'].mean())\n\n# 6) TF-IDF (sparse)\nmax_features = 5000\nprint(\"Computing TF-IDF (max_features=\", max_features, \") ...\", sep=\"\")\ntfidf = TfidfVectorizer(max_features=max_features, stop_words='english', token_pattern=r'(?u)\\b[A-Za-z0-9_]{2,}\\b', min_df=1)\ntfidf_matrix = tfidf.fit_transform(product_meta['text_clean'].astype(str).values)\nprint(\"TF-IDF shape:\", tfidf_matrix.shape)\n\n# 7) Use SVD to get dense embeddings (lower-dim) for ANN\nn_items = tfidf_matrix.shape[0]\nif n_items <= 0:\n    raise RuntimeError(\"No items to index. Check product_meta.\")\n# choose components\nif n_items > 20000:\n    n_comp = 128\nelif n_items > 5000:\n    n_comp = 96\nelse:\n    n_comp = min(64, max(8, n_items//10))\nprint(\"Using TruncatedSVD with n_components =\", n_comp)\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nitem_embeds = svd.fit_transform(tfidf_matrix)   # dense (n_items x n_comp)\nitem_embeds = normalize(item_embeds, axis=1)\nprint(\"Embeddings shape:\", item_embeds.shape)\n\n# 8) Build Annoy index (angular ~ cosine)\nannoy_dim = item_embeds.shape[1]\nann_index = AnnoyIndex(annoy_dim, metric='angular')\nn_trees = 50 if n_items > 5000 else 20\nprint(f\"Building Annoy index with {n_trees} trees (this may take a moment)...\")\nfor i in range(n_items):\n    ann_index.add_item(i, item_embeds[i].astype('float32'))\nann_index.build(n_trees)\nprint(\"Annoy index built.\")\n\n# 9) Precompute top-K neighbors per item (fast lookups later)\ntop_k = 16\nsimilarity_map = {}\nfor i in range(n_items):\n    # get neighbors (self included as first result)\n    nn = ann_index.get_nns_by_item(i, top_k+1, include_distances=False)\n    # exclude self\n    nn = [j for j in nn if j != i][:top_k]\n    similarity_map[product_meta.at[i,'product_id']] = [product_meta.at[j,'product_id'] for j in nn]\n# show sample\nsample_items = list(similarity_map.keys())[:3]\nprint(\"Sample similarity_map entries (first 3):\")\nfor s in sample_items:\n    print(\" \", s, \"->\", similarity_map[s][:6])\n\n# 10) Save artifacts\njoblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\njoblib.dump({'svd': svd}, \"svd_model.joblib\")\njoblib.dump(product_meta[['product_id','title','text_clean']].reset_index(drop=True), \"product_meta.joblib\")\nann_index.save(\"annoy_index.ann\")\njoblib.dump(similarity_map, \"similarity_map.joblib\")\nprint(\"Saved artifacts: tfidf_vectorizer.joblib, svd_model.joblib, product_meta.joblib, annoy_index.ann, similarity_map.joblib\")\n\nelapsed = time.time() - start\nprint(f\"Done in {elapsed:.1f}s. Indexed {n_items} products.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T04:57:06.184074Z","iopub.execute_input":"2025-11-05T04:57:06.184376Z","iopub.status.idle":"2025-11-05T05:00:49.002697Z","shell.execute_reply.started":"2025-11-05T04:57:06.184356Z","shell.execute_reply":"2025-11-05T05:00:49.001970Z"}},"outputs":[{"name":"stdout","text":"Start pipeline...\nLoading CSV: /kaggle/input/amazon-products-dataset/Amazon-Products.csv\nRaw dataframe shape: (551585, 10)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Unnamed: 0                                               name main_category  \\\n0          0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...    appliances   \n1          1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...    appliances   \n2          2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...    appliances   \n\n       sub_category                                              image  \\\n0  Air Conditioners  https://m.media-amazon.com/images/I/31UISB90sY...   \n1  Air Conditioners  https://m.media-amazon.com/images/I/51JFb7FctD...   \n2  Air Conditioners  https://m.media-amazon.com/images/I/51JFb7FctD...   \n\n                                                link ratings no_of_ratings  \\\n0  https://www.amazon.in/Lloyd-Inverter-Convertib...     4.2         2,255   \n1  https://www.amazon.in/LG-Convertible-Anti-Viru...     4.2         2,948   \n2  https://www.amazon.in/LG-Inverter-Convertible-...     4.2         1,206   \n\n  discount_price actual_price  \n0        â‚¹32,999      â‚¹58,990  \n1        â‚¹46,490      â‚¹75,990  \n2        â‚¹34,490      â‚¹61,990  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>name</th>\n      <th>main_category</th>\n      <th>sub_category</th>\n      <th>image</th>\n      <th>link</th>\n      <th>ratings</th>\n      <th>no_of_ratings</th>\n      <th>discount_price</th>\n      <th>actual_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n      <td>appliances</td>\n      <td>Air Conditioners</td>\n      <td>https://m.media-amazon.com/images/I/31UISB90sY...</td>\n      <td>https://www.amazon.in/Lloyd-Inverter-Convertib...</td>\n      <td>4.2</td>\n      <td>2,255</td>\n      <td>â‚¹32,999</td>\n      <td>â‚¹58,990</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n      <td>appliances</td>\n      <td>Air Conditioners</td>\n      <td>https://m.media-amazon.com/images/I/51JFb7FctD...</td>\n      <td>https://www.amazon.in/LG-Convertible-Anti-Viru...</td>\n      <td>4.2</td>\n      <td>2,948</td>\n      <td>â‚¹46,490</td>\n      <td>â‚¹75,990</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n      <td>appliances</td>\n      <td>Air Conditioners</td>\n      <td>https://m.media-amazon.com/images/I/51JFb7FctD...</td>\n      <td>https://www.amazon.in/LG-Inverter-Convertible-...</td>\n      <td>4.2</td>\n      <td>1,206</td>\n      <td>â‚¹34,490</td>\n      <td>â‚¹61,990</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Text source columns used: ['title', 'name']\nNon-empty text rows (before fallback): 551585 / 551585\nFinal product_meta shape: (551585, 13)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  product_id                                              title  \\\n0         P0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...   \n1         P1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...   \n2         P2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...   \n3         P3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...   \n4         P4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...   \n\n                                                text  \n0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...  \n1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...  \n2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...  \n3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...  \n4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P0</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P1</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P2</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P3</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P4</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Average cleaned text length: 154.20481158842244\nComputing TF-IDF (max_features=5000) ...\nTF-IDF shape: (551585, 5000)\nUsing TruncatedSVD with n_components = 128\nEmbeddings shape: (551585, 128)\nBuilding Annoy index with 50 trees (this may take a moment)...\nAnnoy index built.\nSample similarity_map entries (first 3):\n  P0 -> ['P6', 'P7', 'P16', 'P59', 'P69', 'P747']\n  P1 -> ['P3', 'P22', 'P35', 'P233', 'P804', 'P834']\n  P2 -> ['P274', 'P810', 'P243802', 'P248969', 'P339937', 'P101']\nSaved artifacts: tfidf_vectorizer.joblib, svd_model.joblib, product_meta.joblib, annoy_index.ann, similarity_map.joblib\nDone in 222.8s. Indexed 551585 products.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================\n# Robust loader: choose a large CSV, build product_meta + interactions\n# - merges multiple category columns safely\n# - creates synthetic users/interactions if no user data present\n# - builds user_to_index, item_to_index, and train_csr (sparse matrix)\n# ============================================================\n\nimport os, random, pandas as pd, numpy as np\nfrom scipy.sparse import csr_matrix\nrandom.seed(42)\nnp.random.seed(42)\n\n# ======= ADJUST IF YOUR DATASET PATH IS DIFFERENT =======\nDATA_FOLDER = \"/kaggle/input/amazon-products-dataset\"\n# ========================================================\n\n# 1) List CSV files & sizes, prefer largest non-empty CSV\ncsv_files = [f for f in os.listdir(DATA_FOLDER) if f.lower().endswith('.csv')]\nif not csv_files:\n    raise FileNotFoundError(f\"No CSV files found in {DATA_FOLDER}\")\n\nfile_info = []\nfor f in csv_files:\n    path = os.path.join(DATA_FOLDER, f)\n    try:\n        size = os.path.getsize(path)\n        # quick peek to ensure not empty & to show headers\n        sample = pd.read_csv(path, nrows=5)\n        file_info.append((f, size, sample.shape[0], list(sample.columns)))\n    except Exception as e:\n        # skip unreadable files\n        print(f\"Skipping {f}: {e}\")\n\n# Sort by file size descending\nfile_info.sort(key=lambda x: x[1], reverse=True)\nprint(\"Top CSV candidates (name, size_bytes, sample_rows, sample_cols):\")\nfor info in file_info[:10]:\n    print(\" -\", info)\n\n# Pick the largest file that has at least 20 rows in full read, else pick largest available\nchosen_fname = None\nfor f, size, sample_rows, cols in file_info:\n    if size > 1024 and sample_rows > 0:\n        # try full read to check row count (safe; dataset sizes may be large but Kaggle can handle)\n        path = os.path.join(DATA_FOLDER, f)\n        try:\n            df_tmp = pd.read_csv(path)\n            if df_tmp.shape[0] >= 20:\n                chosen_fname = f\n                df = df_tmp\n                break\n        except Exception:\n            continue\n\n# fallback: choose biggest readable file\nif chosen_fname is None:\n    if not file_info:\n        raise FileNotFoundError(\"No readable CSV files in dataset folder.\")\n    chosen_fname = file_info[0][0]\n    df = pd.read_csv(os.path.join(DATA_FOLDER, chosen_fname))\n\nprint(f\"\\nâœ… Chosen file: {chosen_fname} â€” shape {df.shape}\")\nprint(\"Sample columns:\", list(df.columns)[:50])\n\n# 2) Intelligent renaming of common columns\nrename_map = {}\nfor c in df.columns:\n    cl = c.strip().lower()\n    if cl in ['reviewerid','reviewer_id','user','userid','user_id','customerid','customer_id']:\n        rename_map[c] = 'user_id'\n    if cl in ['asin','product','productid','product_id','item','itemid','item_id','sku','id']:\n        # heuristic: prefer ASIN-like or generic id names\n        rename_map[c] = 'product_id'\n    if cl in ['rating','overall','score','stars']:\n        rename_map[c] = 'rating'\n    if cl in ['title','product_title','name','summary','headline']:\n        rename_map[c] = 'title'\n    if cl in ['reviewtext','description','product_description','body','desc']:\n        rename_map[c] = 'description'\n    if 'category' in cl:\n        # capture main_category, sub_category, etc.\n        rename_map[c] = 'category'\n\n# apply rename map (only keys present)\ndf = df.rename(columns=rename_map)\nprint(\"Applied renames (if any):\", rename_map)\nprint(\"Columns after rename (sample):\", list(df.columns)[:50])\n\n# 3) Build product_meta safely (join multiple category columns if present)\n# If there are multiple original category-like columns they will have been renamed to 'category' possibly multiple times;\n# find all original column names that contained 'category' then join them.\ncat_cols = [orig for orig in df.columns if 'category' in orig.lower()]\n# But since we normalized into 'category' above, also check for duplicate columns by original names:\n# Build list of columns from original df (before rename) that contained 'category' is already found above as cat_cols.\n# Use the existing df columns and attempt to gather all category-like columns (heuristic: columns with 'category' or 'cat' in name)\ncat_candidates = [c for c in df.columns if 'category' in c.lower() or c.lower().endswith('cat') or c.lower().endswith('_cat')]\ncat_cols = list(dict.fromkeys(cat_cols + cat_candidates))  # unique preserve order\n\n# Ensure product_id, title, description exist (synthesize if not)\nif 'product_id' not in df.columns:\n    # try to derive from 'link' or index\n    if 'link' in df.columns:\n        df['product_id'] = df['link'].astype(str)\n    else:\n        df['product_id'] = df.index.astype(str).map(lambda x: f\"P{x}\")\n\nif 'title' not in df.columns:\n    # best-effort: use first string-like column\n    for c in df.columns:\n        if df[c].dtype == object and c != 'product_id':\n            df['title'] = df[c].astype(str)\n            break\n    if 'title' not in df.columns:\n        df['title'] = df['product_id'].astype(str)\n\nif 'description' not in df.columns:\n    df['description'] = df['title'].astype(str)\n\n# Build product_meta with a single 'category' column that aggregates category-like columns\nif cat_cols:\n    # convert all cat_cols to string and join them into one text column\n    product_meta = df[['product_id', 'title', 'description']].copy()\n    # some cat_cols may not actually be in df due to prior renames; filter\n    cat_cols_present = [c for c in cat_cols if c in df.columns]\n    if cat_cols_present:\n        product_meta['category'] = df[cat_cols_present].astype(str).agg(' '.join, axis=1)\n    else:\n        product_meta['category'] = \"Unknown\"\nelse:\n    product_meta = df[['product_id', 'title', 'description']].copy()\n    product_meta['category'] = \"Unknown\"\n\nproduct_meta = product_meta.drop_duplicates('product_id').reset_index(drop=True)\n\nprint(f\"\\nâœ… product_meta built: {product_meta.shape[0]} unique products\")\ndisplay(product_meta.head())\n\n# 4) Build interactions\nif 'user_id' in df.columns and 'product_id' in df.columns:\n    print(\"Detected user interactions in the file -> building interactions from real data.\")\n    interactions = df[['user_id', 'product_id']].copy()\n    if 'rating' in df.columns:\n        interactions['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(1.0)\n    else:\n        interactions['rating'] = 1.0\nelse:\n    # No user info -> generate synthetic users with random interactions\n    print(\"No user info found. Generating synthetic users & interactions.\")\n    n_products = product_meta.shape[0]\n    if n_products == 0:\n        raise RuntimeError(\"No products available in chosen CSV to generate interactions. Pick a different file.\")\n    n_users = 800\n    interactions_list = []\n    all_products = product_meta['product_id'].tolist()\n    for u in range(n_users):\n        # each synthetic user interacts with 10-25 random products\n        k = min(len(all_products), random.randint(10, min(25, max(10, len(all_products)//10))))\n        chosen = random.sample(all_products, k=k)\n        for pid in chosen:\n            interactions_list.append({'user_id': f\"U{u}\", 'product_id': pid, 'rating': 1.0})\n    interactions = pd.DataFrame(interactions_list)\n\nprint(f\"\\nâœ… interactions built (before pruning): {interactions.shape[0]} rows\")\ndisplay(interactions.head())\n\n# 5) Light pruning to remove extremely rare users/items (set thresholds to 1 to be permissive)\nmin_user_interactions = 1\nmin_item_interactions = 1\n\nuser_counts = interactions['user_id'].value_counts()\nvalid_users = user_counts[user_counts >= min_user_interactions].index\nitem_counts = interactions['product_id'].value_counts()\nvalid_items = item_counts[item_counts >= min_item_interactions].index\n\ninteractions = interactions[\n    interactions['user_id'].isin(valid_users) & interactions['product_id'].isin(valid_items)\n].copy()\n\nprint(\"After pruning interactions:\", interactions.shape)\n\n# 6) Filter product_meta to only products present in interactions\nproduct_meta = product_meta[product_meta['product_id'].isin(interactions['product_id'].unique())].reset_index(drop=True)\nprint(\"Filtered product_meta shape:\", product_meta.shape)\n\n# 7) Build mappings and sparse matrix (train_csr)\nuser_ids = interactions['user_id'].unique().tolist()\nitem_ids = interactions['product_id'].unique().tolist()\nuser_to_index = {u:i for i,u in enumerate(user_ids)}\nindex_to_user = {i:u for u,i in user_to_index.items()}\nitem_to_index = {p:i for i,p in enumerate(item_ids)}\nindex_to_item = {i:p for p,i in item_to_index.items()}\n\nrows = interactions['user_id'].map(user_to_index)\ncols = interactions['product_id'].map(item_to_index)\ndata = interactions['rating'].astype(float)\n\ntrain_csr = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids)))\n\nprint(\"\\nâœ… Final diagnostics:\")\nprint(\" - chosen CSV:\", chosen_fname)\nprint(\" - interactions rows:\", interactions.shape[0])\nprint(\" - unique users:\", len(user_ids))\nprint(\" - unique items:\", len(item_ids))\nprint(\" - train_csr shape (users x items):\", train_csr.shape)\n\n# variables available for next steps: interactions, product_meta, user_to_index, item_to_index, train_csr\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:02:51.981936Z","iopub.execute_input":"2025-11-05T05:02:51.982618Z","iopub.status.idle":"2025-11-05T05:02:58.370129Z","shell.execute_reply.started":"2025-11-05T05:02:51.982591Z","shell.execute_reply":"2025-11-05T05:02:58.369369Z"}},"outputs":[{"name":"stdout","text":"Top CSV candidates (name, size_bytes, sample_rows, sample_cols):\n - ('Amazon-Products.csv', 188603963, 5, ['Unnamed: 0', 'name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Fashion and Silver Jewellery.csv', 7045316, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Jewellery.csv', 6730217, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Lingerie and Nightwear.csv', 6609118, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Bags and Luggage.csv', 6563675, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Clothing.csv', 6402549, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Ethnic Wear.csv', 6344238, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Handbags and Clutches.csv', 6260299, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Mens Fashion.csv', 6193234, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n - ('Watches.csv', 6151619, 5, ['name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price'])\n\nâœ… Chosen file: Amazon-Products.csv â€” shape (551585, 10)\nSample columns: ['Unnamed: 0', 'name', 'main_category', 'sub_category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\nApplied renames (if any): {'name': 'title', 'main_category': 'category', 'sub_category': 'category'}\nColumns after rename (sample): ['Unnamed: 0', 'title', 'category', 'category', 'image', 'link', 'ratings', 'no_of_ratings', 'discount_price', 'actual_price']\n\nâœ… product_meta built: 551585 unique products\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                          product_id  \\\n0  https://www.amazon.in/Lloyd-Inverter-Convertib...   \n1  https://www.amazon.in/LG-Convertible-Anti-Viru...   \n2  https://www.amazon.in/LG-Inverter-Convertible-...   \n3  https://www.amazon.in/LG-Convertible-Anti-Viru...   \n4  https://www.amazon.in/Carrier-Inverter-Split-C...   \n\n                                               title  \\\n0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...   \n1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...   \n2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...   \n3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...   \n4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...   \n\n                                         description  \\\n0  Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...   \n1  LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...   \n2  LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...   \n3  LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...   \n4  Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...   \n\n                      category  \n0  appliances Air Conditioners  \n1  appliances Air Conditioners  \n2  appliances Air Conditioners  \n3  appliances Air Conditioners  \n4  appliances Air Conditioners  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.amazon.in/Lloyd-Inverter-Convertib...</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n      <td>Lloyd 1.5 Ton 3 Star Inverter Split Ac (5 In 1...</td>\n      <td>appliances Air Conditioners</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.amazon.in/LG-Convertible-Anti-Viru...</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 5 Star AI DUAL Inverter Split AC (C...</td>\n      <td>appliances Air Conditioners</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.amazon.in/LG-Inverter-Convertible-...</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n      <td>LG 1 Ton 4 Star Ai Dual Inverter Split Ac (Cop...</td>\n      <td>appliances Air Conditioners</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.amazon.in/LG-Convertible-Anti-Viru...</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n      <td>LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (C...</td>\n      <td>appliances Air Conditioners</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.amazon.in/Carrier-Inverter-Split-C...</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n      <td>Carrier 1.5 Ton 3 Star Inverter Split AC (Copp...</td>\n      <td>appliances Air Conditioners</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"No user info found. Generating synthetic users & interactions.\n\nâœ… interactions built (before pruning): 14046 rows\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  user_id                                         product_id  rating\n0      U0  https://www.amazon.in/Shrey-SHREY-Rucksack-Bag...     1.0\n1      U0  https://www.amazon.in/MERSODA%C2%AE-Stylish-Co...     1.0\n2      U0  https://www.amazon.in/AmazonBasics-Lightning-C...     1.0\n3      U0  https://www.amazon.in/Zebronics-Zeb-Symphony-B...     1.0\n4      U0  https://www.amazon.in/BIBA-Womens-Anarkali-SKD...     1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/Shrey-SHREY-Rucksack-Bag...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/MERSODA%C2%AE-Stylish-Co...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/AmazonBasics-Lightning-C...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/Zebronics-Zeb-Symphony-B...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U0</td>\n      <td>https://www.amazon.in/BIBA-Womens-Anarkali-SKD...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"After pruning interactions: (14046, 3)\nFiltered product_meta shape: (13873, 4)\n\nâœ… Final diagnostics:\n - chosen CSV: Amazon-Products.csv\n - interactions rows: 14046\n - unique users: 800\n - unique items: 13873\n - train_csr shape (users x items): (800, 13873)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================\n# ğŸ§  Content-based encoding (TF-IDF)\n# ============================================================\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Combine title + description + category into one text column\ndef combine_text(row):\n    return \" \".join(str(x) for x in [row.get('title',''), row.get('description',''), row.get('category','')] if pd.notna(x))\n\nproduct_meta['text'] = product_meta.apply(combine_text, axis=1)\n\ntfidf = TfidfVectorizer(max_features=5000, stop_words='english')\ntfidf_matrix = tfidf.fit_transform(product_meta['text'].fillna(''))\nprod_cosine = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\nprint(\"âœ… TF-IDF matrix shape:\", tfidf_matrix.shape)\nprint(\"âœ… Cosine similarity shape:\", prod_cosine.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:02:58.371289Z","iopub.execute_input":"2025-11-05T05:02:58.371543Z","iopub.status.idle":"2025-11-05T05:03:01.869497Z","shell.execute_reply.started":"2025-11-05T05:02:58.371525Z","shell.execute_reply":"2025-11-05T05:03:01.868661Z"}},"outputs":[{"name":"stdout","text":"âœ… TF-IDF matrix shape: (13873, 5000)\nâœ… Cosine similarity shape: (13873, 13873)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================================\n# ğŸ§© Collaborative filtering with Truncated SVD\n# ============================================================\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\n\nn_users, n_items = train_csr.shape\nif n_items < 2:\n    raise ValueError(f\"âŒ Too few unique products ({n_items}). Pick a larger CSV or lower pruning.\")\n\n# choose components safely\nn_components = min(50, max(2, min(n_users, n_items) - 1))\nprint(f\"Using {n_components} latent components\")\n\nsvd = TruncatedSVD(n_components=n_components, random_state=42)\nuser_factors = svd.fit_transform(train_csr)\nitem_factors = svd.components_.T\n\n# normalize for cosine comparisons\nuser_factors = normalize(user_factors, axis=1)\nitem_factors = normalize(item_factors, axis=1)\n\nprint(\"âœ… user_factors:\", user_factors.shape)\nprint(\"âœ… item_factors:\", item_factors.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:03:07.189520Z","iopub.execute_input":"2025-11-05T05:03:07.189791Z","iopub.status.idle":"2025-11-05T05:03:07.272689Z","shell.execute_reply.started":"2025-11-05T05:03:07.189771Z","shell.execute_reply":"2025-11-05T05:03:07.271835Z"}},"outputs":[{"name":"stdout","text":"Using 50 latent components\nâœ… user_factors: (800, 50)\nâœ… item_factors: (13873, 50)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================\n# ğŸ§  Hybrid Recommendation System\n# ============================================================\n\nimport numpy as np\n\n# mapping index to product_id\nindex_to_item_list = [index_to_item[i] for i in range(len(index_to_item))]\nprodid_to_idx = {pid: i for i, pid in enumerate(product_meta['product_id'].values)}\n\ndef recommend_for_user(user_id, top_k=10, alpha=0.6):\n    \"\"\"\n    alpha â†’ weight for collaborative filtering (0â€“1)\n    0 = pure content-based, 1 = pure collaborative\n    \"\"\"\n    if user_id not in user_to_index:\n        return []\n    uidx = user_to_index[user_id]\n    coll_scores = item_factors.dot(user_factors[uidx])\n    \n    # Compute content-based similarity (mean of items the user interacted with)\n    user_items = interactions[interactions['user_id'] == user_id]['product_id'].unique().tolist()\n    content_scores = np.zeros(len(item_ids))\n    for pid in user_items:\n        if pid in prodid_to_idx:\n            idx = prodid_to_idx[pid]\n            sim = prod_cosine[idx]\n            content_scores += sim\n    if len(user_items) > 0:\n        content_scores /= len(user_items)\n\n    # Normalize and combine\n    coll_norm = (coll_scores - coll_scores.min()) / (coll_scores.max() - coll_scores.min() + 1e-9)\n    cont_norm = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min() + 1e-9)\n    hybrid = alpha * coll_norm + (1 - alpha) * cont_norm\n\n    # Exclude items already seen\n    seen = set(user_items)\n    candidates = [(index_to_item_list[i], hybrid[i]) for i in range(len(hybrid)) if index_to_item_list[i] not in seen]\n    candidates.sort(key=lambda x: x[1], reverse=True)\n\n    return [pid for pid, score in candidates[:top_k]]\n\n# Try one random user\nsample_user = np.random.choice(interactions['user_id'].unique())\nrecs = recommend_for_user(sample_user, top_k=10, alpha=0.6)\nprint(f\"Example user: {sample_user}\")\nprint(\"Top Recommendations:\", recs[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:03:10.919139Z","iopub.execute_input":"2025-11-05T05:03:10.919861Z","iopub.status.idle":"2025-11-05T05:03:10.956029Z","shell.execute_reply.started":"2025-11-05T05:03:10.919839Z","shell.execute_reply":"2025-11-05T05:03:10.955466Z"}},"outputs":[{"name":"stdout","text":"Example user: U102\nTop Recommendations: ['https://www.amazon.in/Paninaro-Afghani-Oxidised-Silver-Necklace/dp/B09746PB5S/ref=sr_1_18103?qid=1679160577&s=jewelry&sr=1-18103', 'https://www.amazon.in/Liberty-Fortune-VCL-3-Formal-Shoes/dp/B0BVZ4Y5RB/ref=sr_1_8658?qid=1679148215&s=shoes&sr=1-8658', 'https://www.amazon.in/SS-Art-Land-Decorations-Activities/dp/B0BHPCRHNN/ref=sr_1_7960?qid=1679215230&s=beauty&sr=1-7960', 'https://www.amazon.in/INDO-ERA-Straight-Trouser-Blue_RRRRR1996_Medium/dp/B097KCSH1S/ref=sr_1_7187?qid=1679152999&s=apparel&sr=1-7187', 'https://www.amazon.in/Peter-England-Cotton-Classic-PETRMRGB249085_Navy_XL/dp/B0B575L4ZY/ref=sr_1_295?qid=1679142483&s=apparel&sr=1-295', 'https://www.amazon.in/Sukkhi-Classic-Plated-Necklace-CBMIX104228/dp/B09Y3D2QWC/ref=sr_1_13006?qid=1679160394&s=jewelry&sr=1-13006', 'https://www.amazon.in/PRO-GYM-Compression-T-Shirt-Athletic/dp/B08VW4T7DP/ref=sr_1_3264?qid=1679217401&s=sports&sr=1-3264', 'https://www.amazon.in/Lining-Airforce-Badminton-Racquet-Cover/dp/B09CTG2J4F/ref=sr_1_9099?qid=1679216981&s=sports&sr=1-9099', 'https://www.amazon.in/Zeal-Checkered-Cotton-1159-X-Large/dp/B0BMG397J1/ref=sr_1_14723?qid=1679141434&s=apparel&sr=1-14723', 'https://www.amazon.in/L-Louges-Analog-Watch-Women/dp/B0B5N2LBKR/ref=sr_1_18787?qid=1679156291&s=watches&sr=1-18787']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================\n# ğŸ“Š Evaluate model using Precision@K and Recall@K\n# ============================================================\n\ndef precision_at_k(recs, gt, k):\n    if not recs or not gt:\n        return 0\n    return len(set(recs[:k]) & set(gt)) / k\n\ndef recall_at_k(recs, gt, k):\n    if not recs or not gt:\n        return 0\n    return len(set(recs[:k]) & set(gt)) / len(gt)\n\n# Split small test set: hold out one item per user\ntest_df = interactions.groupby('user_id').head(1)\ntrain_df = interactions[~interactions.index.isin(test_df.index)]\n\ndef evaluate(test_df, k=10, alpha=0.6):\n    users = test_df['user_id'].unique()\n    precisions, recalls = [], []\n    for u in users:\n        gt = test_df[test_df['user_id'] == u]['product_id'].tolist()\n        recs = recommend_for_user(u, top_k=k, alpha=alpha)\n        precisions.append(precision_at_k(recs, gt, k))\n        recalls.append(recall_at_k(recs, gt, k))\n    return np.mean(precisions), np.mean(recalls)\n\nfor alpha in [0.0, 0.3, 0.5, 0.7, 1.0]:\n    p, r = evaluate(test_df, k=10, alpha=alpha)\n    print(f\"Î±={alpha:.1f} â†’ Precision@10={p:.4f}, Recall@10={r:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:03:17.062852Z","iopub.execute_input":"2025-11-05T05:03:17.063122Z","iopub.status.idle":"2025-11-05T05:04:06.938980Z","shell.execute_reply.started":"2025-11-05T05:03:17.063102Z","shell.execute_reply":"2025-11-05T05:04:06.938254Z"}},"outputs":[{"name":"stdout","text":"Î±=0.0 â†’ Precision@10=0.0000, Recall@10=0.0000\nÎ±=0.3 â†’ Precision@10=0.0000, Recall@10=0.0000\nÎ±=0.5 â†’ Precision@10=0.0000, Recall@10=0.0000\nÎ±=0.7 â†’ Precision@10=0.0000, Recall@10=0.0000\nÎ±=1.0 â†’ Precision@10=0.0000, Recall@10=0.0000\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================\n# ğŸ’¾ Save model artifacts\n# ============================================================\nimport joblib\n\njoblib.dump(tfidf, \"/kaggle/working/tfidf_vectorizer.joblib\")\njoblib.dump(product_meta, \"/kaggle/working/product_meta.joblib\")\njoblib.dump({\n    \"user_factors\": user_factors,\n    \"item_factors\": item_factors,\n    \"user_to_index\": user_to_index,\n    \"index_to_item\": index_to_item\n}, \"/kaggle/working/svd_factors.joblib\")\n\nprint(\"âœ… Artifacts saved in /kaggle/working/\")\nprint(\"Files ready for Streamlit deployment:\")\n!ls -lh /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:06:16.925896Z","iopub.execute_input":"2025-11-05T05:06:16.926560Z","iopub.status.idle":"2025-11-05T05:06:17.295382Z","shell.execute_reply.started":"2025-11-05T05:06:16.926535Z","shell.execute_reply":"2025-11-05T05:06:17.294374Z"}},"outputs":[{"name":"stdout","text":"âœ… Artifacts saved in /kaggle/working/\nFiles ready for Streamlit deployment:\ntotal 769M\n-rw-r--r-- 1 root root 701M Nov  5 05:00 annoy_index.ann\n-rw-r--r-- 1 root root 6.2M Nov  5 05:06 product_meta.joblib\n-rw-r--r-- 1 root root  50M Nov  5 05:00 similarity_map.joblib\n-rw-r--r-- 1 root root 7.4M Nov  5 05:06 svd_factors.joblib\n-rw-r--r-- 1 root root 4.9M Nov  5 05:00 svd_model.joblib\n-rw-r--r-- 1 root root 176K Nov  5 05:06 tfidf_vectorizer.joblib\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for u in random.sample(interactions['user_id'].unique().tolist(), 3):\n    recs = recommend_for_user(u, top_k=5, alpha=0.6)\n    print(f\"\\nUser {u} â†’ Recommendations:\")\n    print(product_meta[product_meta['product_id'].isin(recs)][['title','category']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:06:20.232930Z","iopub.execute_input":"2025-11-05T05:06:20.233733Z","iopub.status.idle":"2025-11-05T05:06:20.311161Z","shell.execute_reply.started":"2025-11-05T05:06:20.233705Z","shell.execute_reply":"2025-11-05T05:06:20.310501Z"}},"outputs":[{"name":"stdout","text":"\nUser U166 â†’ Recommendations:\n                                                   title  \\\n296    GM 3012 3 Pin Travel Universal Multi-Plug with...   \n999    Lee Cooper Unisex Polyester Super Breathable a...   \n7562                 VMart Men Cotton Medium Waist Jeans   \n10891                              Allen Solly Men Shirt   \n11192  Shopnet Wireless Bluetooth Speaker TG113 For M...   \n\n                                  category  \n296    tv, audio & cameras All Electronics  \n999             accessories Bags & Luggage  \n7562                  men's clothing Jeans  \n10891                men's clothing Shirts  \n11192         tv, audio & cameras Speakers  \n\nUser U149 â†’ Recommendations:\n                                                   title  \\\n5187   Avsar 18k (750) Yellow Gold and Diamond Pendan...   \n6641   Home9ine Matte Poly Duck Red Color Digital Pri...   \n10260  CuTech New 1080p HD Bulb Light Camera Fish Eye...   \n10676                             Jack & Jones Men Shirt   \n13441  Style Quotient Women Pretty Pink Gingham Check...   \n\n                                   category  \n5187   accessories Gold & Diamond Jewellery  \n6641         home & kitchen Home Furnishing  \n10260  tv, audio & cameras Security Cameras  \n10676                 men's clothing Shirts  \n13441         women's clothing Western Wear  \n\nUser U350 â†’ Recommendations:\n                                                  title  \\\n159   TEâ„¢ Instant Electric Heating Hot and Cold Wate...   \n5400  ZOUK Multicolored Fabric Women's Handmade Wall...   \n6414  Wayona USB Type C Cable with QC 3.0 18W Fast C...   \n7099  SanashaÂ® Men's Stylish & Comfortable Polyester...   \n9089          YKI Women's Synthetic Printed Maxi Nighty   \n\n                                            category  \n159                        appliances All Appliances  \n5400                 accessories Handbags & Clutches  \n6414  tv, audio & cameras Home Entertainment Systems  \n7099                        men's clothing Innerwear  \n9089           women's clothing Lingerie & Nightwear  \n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def train_test_overlap(df, user_col='user_id', item_col='product_id'):\n    \"\"\"Guarantee that held-out items exist in training set.\"\"\"\n    train_df, test_df = [], []\n    for u, group in df.groupby(user_col):\n        if len(group) <= 2:\n            train_df.append(group)\n        else:\n            test_df.append(group.sample(1))\n            train_df.append(group.drop(test_df[-1].index))\n    train_df = pd.concat(train_df)\n    test_df = pd.concat(test_df)\n    return train_df, test_df\n\ntrain_df, test_df = train_test_overlap(interactions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:06:23.958282Z","iopub.execute_input":"2025-11-05T05:06:23.958817Z","iopub.status.idle":"2025-11-05T05:06:24.205972Z","shell.execute_reply.started":"2025-11-05T05:06:23.958797Z","shell.execute_reply":"2025-11-05T05:06:24.205164Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# View some available synthetic users\nlist(interactions['user_id'].unique())[:20]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:06:26.471156Z","iopub.execute_input":"2025-11-05T05:06:26.471487Z","iopub.status.idle":"2025-11-05T05:06:26.477974Z","shell.execute_reply.started":"2025-11-05T05:06:26.471454Z","shell.execute_reply":"2025-11-05T05:06:26.477351Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['U0',\n 'U1',\n 'U2',\n 'U3',\n 'U4',\n 'U5',\n 'U6',\n 'U7',\n 'U8',\n 'U9',\n 'U10',\n 'U11',\n 'U12',\n 'U13',\n 'U14',\n 'U15',\n 'U16',\n 'U17',\n 'U18',\n 'U19']"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"product_meta[['product_id', 'title']].head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:06:29.210994Z","iopub.execute_input":"2025-11-05T05:06:29.211268Z","iopub.status.idle":"2025-11-05T05:06:29.232343Z","shell.execute_reply.started":"2025-11-05T05:06:29.211249Z","shell.execute_reply":"2025-11-05T05:06:29.231699Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                          product_id  \\\n0  https://www.amazon.in/Voltas-Inverter-Split-Co...   \n1  https://www.amazon.in/Blue-Star-Portable-Coppe...   \n2  https://www.amazon.in/Panasonic-Convertible-ad...   \n3  https://www.amazon.in/Daikin-Inverter-Copper-F...   \n4  https://www.amazon.in/Hisense-Inverter-Copper-...   \n5  https://www.amazon.in/Voltas-Split-Hot-Cold-Wh...   \n6  https://www.amazon.in/Daikin-Inverter-Copper-F...   \n7  https://www.amazon.in/Voltas-Inverter-Copper-1...   \n8  https://www.amazon.in/Voltas-Adjustable-Invert...   \n9  https://www.amazon.in/Voltas-Split-Copper-183D...   \n\n                                               title  \n0  Voltas 1.5 Ton, 5 Star, Inverter Split AC(Copp...  \n1  Blue Star 1 Ton Fixed Speed Portable AC (Coppe...  \n2  Panasonic 2 Ton 5 Star Wi-Fi Inverter Smart Sp...  \n3  Daikin 1.5 Ton 5 Star Inverter Split AC (Coppe...  \n4  Hisense 1.0 Ton 5 Star Inverter Split AC (Copp...  \n5  Voltas 1.5 Ton Antibacterial Coating, Heater, ...  \n6  Daikin 1.8 Ton 5 Star Inverter Split AC (Coppe...  \n7  Voltas 1 Ton 3 Star Inverter Split AC (Copper ...  \n8  Voltas 1.5 Ton 3 Star Adjustable Inverter Copp...  \n9  Voltas 1.5 Ton 3 Star Non-Inverter Split AC (1...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.amazon.in/Voltas-Inverter-Split-Co...</td>\n      <td>Voltas 1.5 Ton, 5 Star, Inverter Split AC(Copp...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.amazon.in/Blue-Star-Portable-Coppe...</td>\n      <td>Blue Star 1 Ton Fixed Speed Portable AC (Coppe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.amazon.in/Panasonic-Convertible-ad...</td>\n      <td>Panasonic 2 Ton 5 Star Wi-Fi Inverter Smart Sp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.amazon.in/Daikin-Inverter-Copper-F...</td>\n      <td>Daikin 1.5 Ton 5 Star Inverter Split AC (Coppe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.amazon.in/Hisense-Inverter-Copper-...</td>\n      <td>Hisense 1.0 Ton 5 Star Inverter Split AC (Copp...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>https://www.amazon.in/Voltas-Split-Hot-Cold-Wh...</td>\n      <td>Voltas 1.5 Ton Antibacterial Coating, Heater, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>https://www.amazon.in/Daikin-Inverter-Copper-F...</td>\n      <td>Daikin 1.8 Ton 5 Star Inverter Split AC (Coppe...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>https://www.amazon.in/Voltas-Inverter-Copper-1...</td>\n      <td>Voltas 1 Ton 3 Star Inverter Split AC (Copper ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>https://www.amazon.in/Voltas-Adjustable-Invert...</td>\n      <td>Voltas 1.5 Ton 3 Star Adjustable Inverter Copp...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>https://www.amazon.in/Voltas-Split-Copper-183D...</td>\n      <td>Voltas 1.5 Ton 3 Star Non-Inverter Split AC (1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Rebuild a small Annoy index (<= ~25MB) from existing artifacts or product_meta\nimport os, joblib, time, gzip, shutil\nimport numpy as np, pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\n\nstart = time.time()\nprint(\"Start: rebuild compact Annoy index\")\n\n# Parameters (tune if needed)\nTARGET_DIM = 32        # final embedding dimension (32 usually enough)\nN_TREES = 10           # number of Annoy trees (smaller -> smaller file, less accuracy)\nANN_FILENAME = \"annoy_index_small.ann\"\nANN_GZ = ANN_FILENAME + \".gz\"\nSVD_MODEL_SMALL = \"svd_model_small.joblib\"\nPRODUCT_META_SMALL = \"product_meta_small.joblib\"\nTFIDF_NAME = \"tfidf_vectorizer.joblib\"\n\n# 1) Load product_meta (try joblib first, then look for a DataFrame in memory)\nproduct_meta = None\nif os.path.exists(\"product_meta.joblib\"):\n    try:\n        product_meta = joblib.load(\"product_meta.joblib\")\n        print(\"Loaded product_meta.joblib\")\n    except Exception as e:\n        print(\"Failed to load product_meta.joblib:\", e)\nif product_meta is None and 'product_meta' in globals():\n    product_meta = globals()['product_meta']\n    print(\"Using product_meta from notebook globals\")\n\nif product_meta is None:\n    raise RuntimeError(\"product_meta not found. Place product_meta.joblib in working dir or define product_meta in the notebook.\")\n\n# 2) Ensure there is a text column (prefer text_clean > text > title > name)\ntext_col = None\nfor cand in ['text_clean','text','title','name','product_name']:\n    if cand in product_meta.columns:\n        text_col = cand\n        break\n\n# If no candidate, try to build 'text' by joining obvious columns\nif text_col is None:\n    print(\"No text-like column found. Building 'text' from object columns...\")\n    # choose up to 3 object columns that are not likely urls/images\n    obj_cols = [c for c in product_meta.columns if product_meta[c].dtype == 'object']\n    def is_url_col(col):\n        sample = product_meta[col].dropna().astype(str).head(6).tolist()\n        if not sample: return False\n        return sum(1 for s in sample if any(x in s.lower() for x in ('http','www.','.jpg','.png','/dp/','amazon'))) >= 1\n    obj_cols = [c for c in obj_cols if not is_url_col(c)]\n    if not obj_cols:\n        # fallback: use product_id as token\n        if 'product_id' in product_meta.columns:\n            product_meta['text'] = product_meta['product_id'].astype(str)\n            text_col = 'text'\n        else:\n            raise RuntimeError(\"No text columns available and no product_id to fallback to.\")\n    else:\n        chosen = obj_cols[:3]\n        print(\"Using columns to build text:\", chosen)\n        def build_text(row):\n            parts = []\n            for c in chosen:\n                v = row.get(c)\n                if pd.isna(v): continue\n                s = str(v).strip()\n                if s and not s.lower().startswith(\"http\"):\n                    parts.append(s)\n            return \" \".join(parts)\n        product_meta['text'] = product_meta.apply(build_text, axis=1)\n        text_col = 'text'\n\nprint(\"Using text column:\", text_col)\n# create text_clean if missing\nif 'text_clean' not in product_meta.columns:\n    import re\n    def clean_text(s):\n        if pd.isna(s): return \"\"\n        s = str(s).lower()\n        s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n        s = re.sub(r'\\s+', ' ', s).strip()\n        return s\n    product_meta['text_clean'] = product_meta[text_col].fillna(\"\").map(clean_text)\n\nn_items = len(product_meta)\nif n_items == 0:\n    raise RuntimeError(\"product_meta is empty (0 rows). Nothing to index.\")\n\nprint(f\"Products to index: {n_items}; sample cleaned text:\")\nprint(product_meta['text_clean'].astype(str).head(3).tolist())\n\n# 3) Load TF-IDF if available, else fit a small one on current product_meta\ntfidf = None\nif os.path.exists(TFIDF_NAME):\n    try:\n        tfidf = joblib.load(TFIDF_NAME)\n        print(\"Loaded existing TF-IDF vectorizer:\", TFIDF_NAME)\n    except Exception as e:\n        print(\"Failed to load TF-IDF from joblib:\", e)\nif tfidf is None:\n    print(\"Fitting new TF-IDF vectorizer (max_features=5000)...\")\n    tfidf = TfidfVectorizer(max_features=5000, stop_words='english', token_pattern=r'(?u)\\b[A-Za-z0-9_]{2,}\\b', min_df=1)\n    tfidf.fit(product_meta['text_clean'].astype(str).values)\n    joblib.dump(tfidf, TFIDF_NAME)\n    print(\"Saved new TF-IDF to\", TFIDF_NAME)\n\n# 4) Transform to sparse TF-IDF\nprint(\"Transforming text to TF-IDF (sparse)...\")\ntfidf_matrix = tfidf.transform(product_meta['text_clean'].astype(str).values)\nprint(\"TF-IDF shape:\", tfidf_matrix.shape)\n\n# 5) Compute compact SVD embeddings\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\n\nn_comp = TARGET_DIM\nprint(f\"Computing TruncatedSVD with n_components={n_comp} ... (dense embedding)\")\nsvd_small = TruncatedSVD(n_components=n_comp, random_state=42)\nitem_embeds_small = svd_small.fit_transform(tfidf_matrix)  # (n_items, n_comp)\nitem_embeds_small = normalize(item_embeds_small, axis=1)\nprint(\"Embeddings shape:\", item_embeds_small.shape)\n\n# 6) Build compact Annoy index\ntry:\n    from annoy import AnnoyIndex\nexcept Exception:\n    raise RuntimeError(\"annoy not installed. Run `pip install annoy` in your environment.\")\n\ndim = item_embeds_small.shape[1]\nann = AnnoyIndex(dim, metric='angular')\nprint(f\"Adding {n_items} vectors to Annoy index (dim={dim}) ...\")\nfor i in range(n_items):\n    ann.add_item(i, item_embeds_small[i].astype('float32'))\n\nprint(f\"Building Annoy with {N_TREES} trees (this may take a bit)...\")\nann.build(N_TREES)\nann.save(ANN_FILENAME)\nprint(\"Saved\", ANN_FILENAME)\n\n# 7) gzip-compress the index for GitHub (reduces size)\nprint(\"Gzipping\", ANN_FILENAME, \"->\", ANN_GZ)\nwith open(ANN_FILENAME, 'rb') as f_in, gzip.open(ANN_GZ, 'wb', compresslevel=9) as f_out:\n    shutil.copyfileobj(f_in, f_out)\ngz_size = os.path.getsize(ANN_GZ) / (1024*1024)\norig_size = os.path.getsize(ANN_FILENAME) / (1024*1024)\nprint(f\"Original ANN size: {orig_size:.2f} MB, gzipped: {gz_size:.2f} MB\")\n\n# 8) Save small SVD & small product_meta (to use in deployment)\njoblib.dump({'svd': svd_small}, SVD_MODEL_SMALL)\nproduct_meta[['product_id','title','text_clean']].to_pickle(PRODUCT_META_SMALL)\nprint(\"Saved compact artifacts:\", SVD_MODEL_SMALL, PRODUCT_META_SMALL)\n\nelapsed = time.time() - start\nprint(f\"Done in {elapsed:.1f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:14:07.380041Z","iopub.execute_input":"2025-11-05T05:14:07.380767Z","iopub.status.idle":"2025-11-05T05:14:08.620608Z","shell.execute_reply.started":"2025-11-05T05:14:07.380741Z","shell.execute_reply":"2025-11-05T05:14:08.619901Z"}},"outputs":[{"name":"stdout","text":"Start: rebuild compact Annoy index\nLoaded product_meta.joblib\nUsing text column: text\nProducts to index: 13873; sample cleaned text:\n['voltas 1 5 ton 5 star inverter split ac copper 4 in 1 adjustable mode anti dust filter 2023 model 185v dazj white voltas 1 5 ton 5 star inverter split ac copper 4 in 1 adjustable mode anti dust filter 2023 model 185v dazj white appliances air conditioners', 'blue star 1 ton fixed speed portable ac copper anti bacterial silver coating self diagnosis comfort sleep modes auto c blue star 1 ton fixed speed portable ac copper anti bacterial silver coating self diagnosis comfort sleep modes auto c appliances air conditioners', 'panasonic 2 ton 5 star wi fi inverter smart split ac copper 7 in 1 convertible with additional ai mode twin cool pm 0 panasonic 2 ton 5 star wi fi inverter smart split ac copper 7 in 1 convertible with additional ai mode twin cool pm 0 appliances air conditioners']\nLoaded existing TF-IDF vectorizer: tfidf_vectorizer.joblib\nTransforming text to TF-IDF (sparse)...\nTF-IDF shape: (13873, 5000)\nComputing TruncatedSVD with n_components=32 ... (dense embedding)\nEmbeddings shape: (13873, 32)\nAdding 13873 vectors to Annoy index (dim=32) ...\nBuilding Annoy with 10 trees (this may take a bit)...\nSaved annoy_index_small.ann\nGzipping annoy_index_small.ann -> annoy_index_small.ann.gz\nOriginal ANN size: 4.16 MB, gzipped: 2.87 MB\nSaved compact artifacts: svd_model_small.joblib product_meta_small.joblib\nDone in 1.2s\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Compact similarity map generator for deployment (GitHub/Streamlit friendly)\nimport joblib, os\nimport numpy as np\nfrom annoy import AnnoyIndex\n\n# --- Parameters ---\nANN_PATH = \"annoy_index_small.ann\"\nOUT_PATH = \"similarity_map_small.joblib\"\nN_NEIGHBORS = 5  # reduce to top 5â€“10 neighbors to shrink file\n\n# --- Load product_meta ---\nif os.path.exists(\"product_meta_small.joblib\"):\n    try:\n        product_meta = joblib.load(\"product_meta_small.joblib\")\n        if isinstance(product_meta, pd.DataFrame):\n            product_meta = product_meta\n        else:\n            product_meta = pd.DataFrame(product_meta)\n    except Exception:\n        product_meta = pd.read_pickle(\"product_meta_small.joblib\")\nelif os.path.exists(\"product_meta.joblib\"):\n    product_meta = joblib.load(\"product_meta.joblib\")\nelse:\n    raise FileNotFoundError(\"product_meta_small.joblib or product_meta.joblib not found!\")\n\n# --- Load Annoy index ---\ndim = 32  # same as you used when building small Annoy index\nann = AnnoyIndex(dim, metric=\"angular\")\nann.load(ANN_PATH)\nprint(f\"âœ… Loaded Annoy index ({dim} dims)\")\n\n# --- Build smaller similarity map ---\nproduct_ids = product_meta['product_id'].astype(str).tolist()\nn_items = len(product_ids)\nsmall_map = {}\n\nprint(f\"Building compact similarity map (top {N_NEIGHBORS} per item, {n_items} items)...\")\nfor i, pid in enumerate(product_ids):\n    nn = ann.get_nns_by_item(i, N_NEIGHBORS + 1)\n    # remove self\n    nn = [j for j in nn if j != i][:N_NEIGHBORS]\n    small_map[pid] = [product_ids[j] for j in nn]\n    if i % 5000 == 0 and i > 0:\n        print(f\"Processed {i}/{n_items} items\")\n\n# --- Save compressed ---\njoblib.dump(small_map, OUT_PATH, compress=('xz', 3))  # strong compression, small size\nsize_mb = os.path.getsize(OUT_PATH) / (1024 * 1024)\nprint(f\"âœ… Saved {OUT_PATH} ({size_mb:.2f} MB)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T05:15:09.549851Z","iopub.execute_input":"2025-11-05T05:15:09.550441Z","iopub.status.idle":"2025-11-05T05:15:10.260122Z","shell.execute_reply.started":"2025-11-05T05:15:09.550418Z","shell.execute_reply":"2025-11-05T05:15:10.259335Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded Annoy index (32 dims)\nBuilding compact similarity map (top 5 per item, 13873 items)...\nProcessed 5000/13873 items\nProcessed 10000/13873 items\nâœ… Saved similarity_map_small.joblib (0.61 MB)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}